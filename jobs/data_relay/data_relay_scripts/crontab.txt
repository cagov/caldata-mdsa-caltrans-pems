# General Background:
# - We manage two primary types of data:
#   1. **VDS30SEC Continuous Data**: This data comes from the following districts: 
#      D3, D4, D5, D6, D7, D8, D10, D11, and D12. It requires periodic uploads 
#      to Snowflake to ensure timely updates for downstream processes.
#   2. **CONFIG Tables**: There are six CONFIG tables (e.g., CONTROLLER_CONFIG, DETECTOR_CONFIG, STATION_CONFIG, 
#       CONTROLLER_CONFIG_LOG, DETECTOR_CONFIG_LOG, STATION_CONFIG_LOG) 
#      that are fully uploaded to Snowflake and refreshed on a daily basis as part of whole-table refresh operations.
#
# - The schedule and batch sizes for these tasks are carefully optimized to meet 
#   downstream requirements, avoid resource constraints, and ensure efficient data handling.
#


#
# =================================
# Cron Jobs (Four):
# =================================
#

# Job 1
# 
# Summit crawl tasks for data relay table pull (Every 3 Hours)
# This job queues crawling tasks for data relay tables by invoking the 
# `append_to_crawl_queue.py` script with the appropriate partition number. 
# It ensures the crawling service on `svgcmdl01.dot.ca.gov` receives timely tasks.
# 
# Note:
# - The `--day_partition_number 8` aligns with the 3-hour intervals (8 partitions in 24 hours): 
#   2 AM, 5 AM, 8 AM, 11 AM, 2 PM, 5 PM, 8 PM, and 11 PM. 
# - This alignment ensures the crawler will receive fresh commands every 3 hours, 
#   helping to spread the load and avoid causing the crawling machine to spike in activity 
#   during certain hours while remaining idle at others. 
# - This scheduling addresses previously observed issues where the machine became extremely busy 
#   and occasionally non-responsive during specific times of the day.
# 
# 

0 2,5,8,11,14,17,20,23 * * * python3.9 /nfsdata/dataop/uploader/append_to_crawl_queue.py --append_to_queue --day_partition_number 8

# Job 2
# 
# Summit crawl tasks for daily config table pull (Daily at 7 PM)
# This job queues tasks for pulling configuration tables using the 
# `append_to_crawl_queue_config.py` script. It ensures the latest configuration
# data is retrieved and ready for processing.
#
# Note:
# - The outcome of this execution adds a workload to the crawler responsible for pulling data 
#   from the Oracle database. The crawled results will be saved under the network drive location: 
#   (i.e. `/nfsdata/dataop/tmp/CONTROLLER_CONFIG/CONTROLLER_CONFIG_static*.parquet`, for the CONTROLLER_CONFIG case, out of the six scenarios)
# - This ensures that the subsequent operation scheduled at 9 PM (the AWS uploading task) 
#   will have the crawled Parquet data ready for upload to Snowflake.
# - The 2-hour interval provides sufficient time for the crawling operation to complete 
#   and ensure the data is available before the uploading task begins.
#

0 19 * * * python3.9 /nfsdata/dataop/uploader/append_to_crawl_queue_config.py --append_to_queue


# Job 3
# 
# Config data upload (Daily at 9 PM)
# This job uploads configuration table data (e.g., CONTROLLER_CONFIG, DETECTOR_CONFIG) 
# to Snowflake using the `aws_upload_daemon_script.sh` shell script. 
# Regular uploads ensure configuration data is consistently synchronized.
#
# Note:
# - This task relies on the results generated by the crawling operation scheduled at 7 PM, 
#   which outputs data as Parquet files in `/nfsdata/dataop/tmp/CONTROLLER_CONFIG/`.
# - By running at 9 PM, the job ensures the crawled data from the Oracle database has 
#   been fully prepared and is ready for upload to Snowflake.
# - The 2-hour interval between crawling and uploading provides sufficient time to account 
#   for the data pull and preparation process.

0 21 * * * sh /nfsdata/dataop/uploader/aws_upload_daemon_script.sh "CONTROLLER_CONFIG,CONTROLLER_CONFIG_LOG,DETECTOR_CONFIG,DETECTOR_CONFIG_LOG,STATION_CONFIG,STATION_CONFIG_LOG"


# Job 4
# 
# VDS30SEC data upload (Every 6 Hours)
# This job uploads VDS30SEC data to Snowflake using the 
# `aws_upload_daemon_script.sh` script. It ensures timely updates for 
# VDS30SEC data at regular 6-hour intervals.
#
# Note:
# - The 6-hour interval is chosen empirically to meet the downstream requirements for 
#   data batch size, which should be around 10 MB in Parquet format according to Snowflake's 
#   best practices.
# - The interval can be adjusted to 24 hours or 1 hour depending on requirements, but with caveats:
#   * **24 Hours**: A 24-hour interval would result in a batch size of approximately 40 MB 
#     in Parquet format, requiring around 5 GB of memory for processing using the PyArrow 
#     backend in Python's Pandas DataFrame. This exceeds the available VM memory, leading 
#     to OutOfMemory errors.
#   * **1 Hour**: A 1-hour interval would create smaller, more fragmented Parquet files, 
#     which could introduce difficulties for downstream processes that prefer larger, 
#     consolidated batches.
# - The 6-hour interval strikes a balance, ensuring manageable memory usage during processing 
#   and creating appropriately sized Parquet files for efficient downstream handling.
#

0 0,6,12,18 * * * sh /nfsdata/dataop/uploader/aws_upload_daemon_script.sh "D3.VDS30SEC,D4.VDS30SEC,D5.VDS30SEC,D6.VDS30SEC,D7.VDS30SEC,D8.VDS30SEC,D10.VDS30SEC,D11.VDS30SEC,D12.VDS30SEC"


