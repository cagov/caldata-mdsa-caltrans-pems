version: 2

macros:
  - name: generate_schema_name
    description: |
      Returns the name of a schema in which a model should land in Snowflake.

      For production models, the value from `dbt_project.yml` will be used, or any
      custom name provided in the model's config block.

      For development models, the schema from `profiles.yml` will be prepended
      to the custom value found in `dbt_project.yml` or the model's config block.

    arguments:
      - name: custom_schema_name
        type: string
        description: |
          The schema provided via `dbt_project.yml` or model config
      - name: node
        type: string
        description: |
          The node that is currently being processed by dbt

  - name: get_snowflake_warehouse
    description: |
      Returns the name of a Snowflake warehouse, given an input size. The dbt target
      (i.e. 'dev' or 'prd') is taken into account when returning a warehouse.
    arguments:
      - name: size
        type: string
        description: |
          Snowflake warehouse size (default is 'XS')

  - name: get_snowflake_refresh_warehouse
    description: |
      Full refreshes of incremental models sometimes need a little bit of
      extra firepower. This returns an appropriate (i.e. 4XL) warehouse
      for that case. Use it with caution! This returns large if either of
      the following conditions are met:

      * The target table doesn't exist (i.e., hasn't been built yet)
      * The `--full-refresh` flag is set via the CLI.
    arguments:
      - name: small
        type: string
        description: |
          Snowflake warehouse size to use when *not* performing a full-refresh
      - name: big
        type: string
        description: |
          Snowflake warehouse size to use when performing a full-refresh

  - name: make_model_incremental
    description: |
      This allows for the implementation of complex incremental model
      logic in one line and one place. Check out [dbt's docs on incremental
      models](https://docs.getdbt.com/docs/build/incremental-models-overview).

      In addition to incrementality, it implements the variable
      "dev_model_look_back", which limits the amount of data selected in dev.
      Two things to note:
        1. Users implement their own date column
        2. This macro does not include a "where" keyword, but does include
        "and". Users are expected to use a "where" clause when implementing this.
    arguments:
      - name: date_col
        type: string
        description: |
          Name of date column to use for limiting selected data

  - name: unload_relation
    description: |
      Unloads a table or view to the project public marts bucket. The resulting file
      will be in a directory matching the schema name of the relation, and will be a
      parquet file with the same columns and data types as the table.

      ### Usage notes:

      1. This is intended for unloading relatively small datasets. Right now,
          it supports partitioning, but not incremental unloads.
      1. This macro is intended to be used as a dbt post-hook.
      1. The name of the external stage is hard-coded. Depending upon the context,
          it will unload to either the prod or dev stage.
      1. The `unload_partitioning` argument can also be provided as a parameter
          in the dbt model config block. This is so that unload partitioning can
          be independently configured for a suite of models that all have the same
          post-hook configured in the `dbt_project.yml`.

    arguments:
      - name: strip_leading_words
        type: integer
        description: |
          How many leading words to remove from the table name when constructing the
          parquet file name. This defaults to one, but for some tables other values
          might be appropriate. For instance, if the table is named
          `performance__station_agg_daily`, then the resulting parquet file will be
          named `station_agg_daily`.
      - name: unload_partitioning
        type: string
        description: |
          If this is set, then the expression in the string will be used as a partitioning
          column for the unloading, and multiple parquet files will be uploaded as a result.
          Each value for the partitioning string will be in the file path for the files,
          so it is a good idea to construct the expression with that in mind. For more documentation,
          see [here](https://docs.snowflake.com/en/sql-reference/sql/copy-into-location)

  - name: timestamp_spine
    description: |
      Generates a table with 1 column; a list of timestamps from a start date to an end date. The
      user can choose a constant spacing between the timestamps (the default is a whole day). The
      spine will include the `start_date`, but not the `end_date`.
    arguments:
      - name: start_date
        type: string
        description: |
          Beginning date for timestamp spine. The day will start at 00:00:00.
      - name: end_date
        type: string
        description: |
          End date for timestamp spine. Note that this date is not included in the output
          spine, only days up until it.
      - name: second_increment
        type: integer
        description: |
          Number of seconds between each timestamp in the spine. Invalid values will result in
          the default value being selected.
      - name: get_county_name
        type: string
        description: |
          This function replace the county numeric code to county full name. The county name was made lower case
          to make it insensitive for kibana search option.
