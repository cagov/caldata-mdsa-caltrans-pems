{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Traffic Operations PeMS Modernization Project","text":"<p>This website contains the technical documentation for Traffic Operations's PeMS Modernization project.</p>"},{"location":"#background","title":"Background","text":"<p>The Performance Measurement System (PeMS) is a mission critical data collection, reporting and analysis product managed by Caltrans Traffic Operations staff. PeMS is also a public facing website, serving worldwide users. Caltrans PeMS is a software tool designed specifically for Caltrans. It is the centralized repository for all of Caltrans\u2019 real-time traffic data, enabling access to these data that might otherwise be dispersed across multiple districts and more difficult to obtain. PeMS provides a consolidated database of traffic data collected by Caltrans placed on state highways throughout California, as well as other Caltrans and partner agency data sets. The data collected by vehicle detectors are relayed from the field to Caltrans Transportation Management Centers (TMCs) and then sent to PeMS. In more technical terms, PeMS is a real-time Archive Data Management System (rt-ADMS) that collects, stores, and processes raw data in real-time. PeMS can be accessed via a standard Internet browser and contains a series of built-in analytical capabilities to support a variety of uses.</p> <p>Due to the current outdated technology stack being used, PeMS has been ranked high as a cyber security risk for Caltrans. The current version of PeMS also contains the following issues:</p> <ol> <li>High maintenance costs</li> <li>Service interruptions</li> <li>Utilizes a sole source contractor</li> <li>Limited access to algorithm's being used in calculations/analysis</li> <li>Data loss</li> <li>Slow user experience</li> </ol> <p>In 2023, Caltrans was awarded with the Modern Data Tech Stack Program by the Office of Data and Innovation (ODI). At no cost to Caltrans, the Office of Data and Innovation (ODI) uses its own budget and staff to develop and deploy the modernized PeMS System. ODI will also train Caltrans Traffic Ops and IT staff to sustainably maintain and further improve the new version of PeMS. The Traffic Operations team is currently pursuing the next phase of the PeMS Modernization Project with ODI, which will feature Modernized PeMS data processing pipeline with Machine Learning model.</p>"},{"location":"architecture/","title":"Project architecture","text":"<p>This project is organized around centralizing data in a Snowflake cloud data warehouse. Heterogeneous data sources are loaded into raw databases using custom Python scripts. These datasets are then transformed into analysis-ready marts using dbt.</p> <p>We follow an adapted version of the project architecture described in this dbt blog post for our Snowflake dbt project.</p> <p>It is described in some detail in ODI CalData's Snowflake docs as well.</p> <pre><code>flowchart TB\nsubgraph External Data Source\n  G[(Geo Data)]\n  CT[(\\nCHP)]\n  TR[(\\nTransit)]\nend\nsubgraph Caltrans VPN\n    direction TB\n    PQ[(\\nDistrict TMC Servers)]\n    C[(\\nOther Data Source)]\n  end\n    PY&gt;Python Scripts]\n    LS&gt;Landing Server]\n    RS&gt;Relay Server]\nsubgraph AWS\n  direction TB\n  A[\\AWS S3 Bucket/]\n  subgraph Caltrans Snowflake\n    subgraph prod environment\n        direction TB\n        RP[(RAW_PRD)]\n        TP[(TRANSFORM_PRD)]\n        AP[(ANALYTICS_PRD)]\n    end\n    subgraph dev environment\n        direction TB\n        RD[(RAW_DEV)]\n        TD[(TRANSFORM_DEV)]\n        AD[(ANALYTICS_DEV)]\n    end\n  end\nend\nP{{PowerBI}}\nPe{{PeMS}}\n\nRS -- LOADER_PRD --&gt; A\nA -- Data Pipelines --&gt; RP\nLS --&gt; RS\nCT --&gt; LS\nTR --&gt; LS\nRD -- TRANSFORMER_DEV --&gt; TD\nTD -- TRANSFORMER_DEV --&gt; AD\nRP -- TRANSFORMER_PRD --&gt; TP\nTP -- TRANSFORMER_PRD --&gt; AP\nRP -- TRANSFORMER_DEV --&gt; TD\nAD -- REPORTER_DEV --&gt; P &amp; Pe\nAP -- REPORTER_PRD --&gt; P &amp; Pe\nAP --&gt; A\nC --&gt; LS\nPQ --&gt; LS\nG --&gt; PY\nPY -- LOADER_PRD --&gt; RP\nPY -- LOADER_DEV --&gt; RD</code></pre>"},{"location":"architecture/#snowflake-architecture","title":"Snowflake architecture","text":"<p>There are two environments set up for this project, development and production. Resources in the development environment are suffixed with <code>DEV</code>, and resources in the production environment are suffixed with <code>PRD</code>.</p> <p>Most of the time, developers will be working in the development environment. Once your feature branches are merged to <code>main</code>, they will be used in the production environment.</p> <p>What follows is a brief description of the most important Snowflake resources in the dev and production environments and how developers are likely to interact with them.</p>"},{"location":"architecture/#six-databases","title":"Six databases","text":"<p>We have six primary databases in our project:</p> <p>Where our Source data lives</p> <ul> <li><code>RAW_DEV</code>: Dev space for loading new source data.</li> <li><code>RAW_PRD</code>: Landing database for production source data.</li> </ul> <p>Where data from our Staging and Intermediate models lives</p> <ul> <li><code>TRANSFORM_DEV</code>: Dev space for staging/intermediate models. This is where most of your dbt work is!</li> <li><code>TRANSFORM_PRD</code>: Prod space for models. This is what builds in the nightly job.</li> </ul> <p>Where data from our Marts models lives</p> <ul> <li><code>ANALYTICS_DEV</code>: Dev space for mart models. Use this when developing a model for a new dashboard or report!</li> <li><code>ANALYTICS_PRD</code>: Prod space for mart models. Point production dashboards and reports to this database.</li> </ul>"},{"location":"architecture/#six-warehouse-groups","title":"Six warehouse groups","text":"<p>There are six warehouse groups for processing data in the databases, corresponding to the primary purposes of the above databases. They are available in a few different sizes, depending upon the needs of the the data processing job, X-small (denoted by (<code>XS</code>), X-Large (denoted by (<code>XL</code>), and 4X-Large (denoted by <code>4XL</code>). Most jobs on small data should use the relevant X-small warehouse.</p> <ol> <li><code>LOADING_{size}_DEV</code>: This warehouse is for loading data to <code>RAW_DEV</code>. It is used for testing new data loading scripts.</li> <li><code>TRANSFORMING_{size}_DEV</code>: This warehouse is for transforming data in <code>TRANSFORM_DEV</code> and <code>ANALYTICS_DEV</code>. Most dbt developers will use this warehouse for daily work.</li> <li><code>REPORTING_{size}_DEV</code>: This warehouse is for testing dashboards.</li> <li><code>LOADING_{size}_PRD</code>: This warehouse is for loading data to <code>RAW_PRD</code>. It is used for production data loading scripts.</li> <li><code>TRANSFORMING_{size}_PRD</code>: This warehouse is for transforming data in <code>TRANSFORM_PRD</code> and <code>ANALYTICS_PRD</code>. This warehouse is used for the nightly builds.</li> <li><code>REPORTING_{size}_PRD</code>: This warehouse is for production dashboards.</li> </ol>"},{"location":"architecture/#six-roles","title":"Six roles","text":"<p>There are six primary functional roles:</p> <ol> <li><code>LOADER_DEV</code>: Dev role for loading data to the <code>RAW_DEV</code> database. This is assumed when developing new data loading scripts.</li> <li> <p><code>LOADER_PRD</code>: Prod role for loading data to the <code>RAW_PRD</code> database. This is assumed by data loading scripts.</p> </li> <li> <p><code>TRANSFORMER_DEV</code>: Dev role for transforming data. This is you! Models built with this role get written to the <code>TRANSFORM_DEV</code> or <code>ANALYTICS_DEV</code> databases. CI robots also use this role to run checks and tests on PRs before they are merged to main.</p> </li> <li> <p><code>TRANSFORMER_PRD</code>: Prod role for transforming data. This is assumed by the nightly build job and writes data to the <code>TRANSFORM_PRD</code> or <code>ANALYTICS_PRD</code> databases.</p> </li> <li> <p><code>REPORTER_DEV</code>: Dev role for reading marts. Use this when developing new dashboards. This role can read models in the <code>ANALYTICS_DEV</code> database.</p> </li> <li><code>REPORTER_PRD</code>: Prod role for reading marts. This is for users and service accounts using production dashboards. This role can read models in the <code>ANALYTICS_PRD</code> database.</li> </ol>"},{"location":"architecture/#reporting-and-analysis","title":"Reporting and analysis","text":"<p>The most prominent consumer of the data products from this project are PowerBI and ArcGis dashboards, the PeMS Server, public users (researchers, private sector, and other public agencies).</p>"},{"location":"architecture/#custom-schema-names","title":"Custom schema names","text":"<p>dbt's default method for generating custom schema names works well for a single-database setup:</p> <ul> <li>It allows development work to occur in a separate schema from production models.</li> <li>It allows analytics engineers to develop side-by-side without stepping on each others toes.</li> </ul> <p>A downside of the default is that production models all get a prefix, which may not be an ideal naming convention for end-users.</p> <p>Because our architecture separates development and production databases, and has strict permissions protecting the <code>RAW_PRD</code> database, there is less danger of breaking production models. So we use our own custom schema name modified from the approach of the GitLab Data Team.</p> <p>In production, each schema is just the custom schema name without any prefix. In non-production environments, dbt developers use their own custom schema based on their name: <code>dbt_username</code>.</p>"},{"location":"architecture/#developing-against-production-data","title":"Developing against production data","text":"<p>Our Snowflake architecture allows for reasonably safe <code>SELECT</code>ing from the production <code>RAW_PRD</code> database while developing models. While this could be expensive for large tables, it also allows for faster and more reliable model development.</p> <p>To develop against raw production data, first you need someone with the <code>USERADMIN</code> role to grant rights to the <code>TRANSFORMER_DEV</code> role (this need only be done once, and can be revoked later):</p> <pre><code>USE ROLE USERADMIN;\nGRANT ROLE RAW_PRD_READ TO ROLE TRANSFORMER_DEV;\n</code></pre>"},{"location":"architecture/#examples","title":"Examples","text":""},{"location":"architecture/#user-personae","title":"User personae","text":"<p>To make the preceding more concrete, let's consider the six databases, <code>RAW</code>, <code>TRANSFORM</code>, and <code>ANALYTICS</code>, for both <code>DEV</code> and <code>PRD</code>:</p> <p></p> <p>If you are a developer, you are doing most of your work in <code>TRANSFORM_DEV</code> and <code>ANALYTICS_DEV</code>, assuming the role <code>TRANSFORMER_DEV</code>. However, you also have the ability to select the production data from <code>RAW_PRD</code> for your development. So your data access looks like the following:</p> <p></p> <p>Now let's consider the nigthly production build. This service account builds the production models in <code>TRANSFORM_PRD</code> and <code>ANALYTICS_PRD</code> based on the raw data in <code>RAW_PRD</code>. The development environment effectively doesn't exist to this account, and data access looks like the following:</p> <p></p> <p>Finally, let's consider an external consumer of a mart from PowerBI. This user has no access to any of the raw or intermediate models (which might contain sensitive data!). To them, the whole rest of the architecture doesn't exist, and they can only see the marts in <code>ANALYTICS_PRD</code>:</p> <p></p>"},{"location":"architecture/#scenario-adding-a-new-data-source","title":"Scenario: adding a new data source","text":"<ol> <li>Write a new Python script (or configure an equivalent loading tool) for loading the data to Snowflake.</li> <li>Assume the <code>LOADER_DEV</code> role and load the data into the <code>RAW_DEV</code> database.</li> <li>Verify that the data was loaded and looks correct in Snowflake.</li> <li>Schedule the Python script to run using the <code>LOADER_PRD</code> role and the <code>RAW_PRD</code> database. This data are now ready for dbt modeling.</li> <li>Once the data is loaded to the <code>RAW_PRD</code> database, drop the data from the <code>RAW_DEV</code> database, it\u2019s not needed anymore.</li> </ol>"},{"location":"architecture/#scenario-creating-a-new-dashboard","title":"Scenario: creating a new dashboard","text":"<ol> <li>Create a branch and develop your new marts table (or modify an existing one) in <code>ANALYTICS_DEV</code> using your normal <code>TRANSFORMER_DEV</code> role.</li> <li>Assume the <code>REPORTER_DEV</code> role with PowerBI and point it to your mart in <code>ANALYTICS_DEV</code>.</li> <li>Once you are happy with your mart and dashboard, merge your branch to main.</li> <li>Once the nightly job is done, your new mart will be built in <code>ANALYTICS_PRD</code>.</li> <li>Assume the <code>REPORTER_PRD</code> role and point your PowerBI dashboard to the production mart.</li> </ol>"},{"location":"data-loading/","title":"Data Loading","text":""},{"location":"data-loading/#prior-state","title":"Prior state","text":"<p>Raw PeMS data are loaded from a variety of servers in Caltrans' network. The existing system involves raw data from the various districts landing on a single server in Caltrans' network.</p> <p>Historically, this data is then picked up by a series of Perl scripts which are transformed into a common schema and loaded into an Oracle database, termed the \"data lake\". Further scripts clean and aggregate the data in the data lake, putting it into another Oracle database termed the \"data warehouse\".</p> <p>The raw data and aggregate metrics are served to the public via the PeMS Website.</p>"},{"location":"data-loading/#data-loading-architecture","title":"Data Loading Architecture","text":"<pre><code>flowchart TB\nCD{Caltrans Districts}\nsubgraph Caltrans Network\n  LCS((Land Closure\\nSystem))\n  CHP((CHP\\nIncidents))\n  L((Data Landing\\nServer))\n  DR((Data Relay\\nServer))\n  C((PeMS\\nWebserver))\n  OD[(Oracle data\\nlake)]\n  OW[(Oracle data\\nwarehouse)]\nend\nsubgraph ODI AWS Account\n    MWAA((AWS Hosted\\nAirflow))\n    S3[S3 Bucket]\nend\nsubgraph Snowflake\n    RAW[(RAW)]\n    TRANSFORM[(TRANSFORM)]\n    ANALYTICS[(ANALYTICS)]\nend\n\nNC((New PeMS\\nWebserver))\n\nOP{Open Data}\nRESEARCH{Researchers}\nSTAFF{Caltrans Traffic\\nOperations}\n\nCD --&gt; L\nC -. Historical\\nData .- MWAA -.-&gt; S3\nL -. Iteris\\nScripts .-&gt; OD\nOD -. Iteris\\nScripts .-&gt; OW\nS3 -- Snowpipe --&gt; RAW\nOW -.-&gt; C\nOD -.-&gt; C\nL --&gt; DR\nDR --&gt; S3\nDR -- Light batching/\\ntransformation --&gt; DR\nRAW --&gt; TRANSFORM --&gt; ANALYTICS\nLCS --&gt; DR\nCHP --&gt; DR\nDR --&gt; RAW\nOD -.-&gt; DR\nOW -.-&gt; DR\n\nANALYTICS --&gt; OP\nANALYTICS --&gt; NC\nNC --&gt; RESEARCH\nNC --&gt; STAFF</code></pre> <p>We are shifting on-premise data pipelines to the cloud, and getting data transformation logic into version control.</p> <p>The above diagram shows both the prior state and the new architecture, which are both running until the new state is completed and adopted. Dotted lines indicate data flow that we hope to deprecate.</p> <p>Note</p> <p>This diagram represents our best current understanding of both Caltrans' network and our planned final state. There may be errors, and not everything is currently implemented.</p>"},{"location":"data-loading/#historical-vehicle-detector-station-data","title":"Historical Vehicle Detector Station Data","text":"<p>Historical vehicle detector station (VDS) files are accessible via the PeMS clearinghouse feeds. We have an instance of AWS managed Airflow (MWAA) which is loading this data to S3 daily. These are then picked up by Snowpipe and loaded to Snowflake for further data transformation.</p> <p>Once we have replaced the ongoing data ingest with the new architecture, this historical ingest feed based on the clearinghouse can be turned off.</p>"},{"location":"data-loading/#ongoing-vehicle-detector-station-data","title":"Ongoing Vehicle Detector Station Data","text":"<p>Raw VDS data from individual districts land on a server within Caltrans as XML files. We are using a \"data relay server\" (Apache Airflow running on a virtual machine within Caltrans' network) to pick up the XML files, transform them to a common schema, aggregate them into daily batches, and upload them to S3. These daily batches are then picked up by Snowpipe and loaded into the <code>RAW</code> database.</p> <p>The data relay server may also load some tables from the Oracle data lake and data warehouse systems, though we intend this to be a temporary pipeline to transfer some auxiliary data, rather than an ongoing one.</p>"},{"location":"data-loading/#lane-closure-system-lcs","title":"Lane Closure System (LCS)","text":"<p>TODO</p>"},{"location":"data-loading/#chp-incident-data","title":"CHP Incident Data","text":"<p>TODO</p>"},{"location":"data-warehouse-performance/","title":"Cloud Data Warehouses","text":""},{"location":"data-warehouse-performance/#what-is-a-cloud-data-warehouse","title":"What is a cloud data warehouse?","text":"<p>Cloud data warehouses (CDWs) are databases which are hosted in the cloud, and are typically optimized around analytical queries like aggregations and window functions, rather than the typical transactional queries that might support a traditional application. This project uses Snowflake as its data warehouse.</p> <p>Cloud data warehouses typically have a few advantages over traditional transactional databases for analytical workflows, including:</p> <ul> <li>They are usually managed services, meaning you don't have to provision and maintain servers.</li> <li>They can scale to truly massive data.</li> </ul> <p>By having a solid understanding of how cloud data warehouses work, you can construct fast, efficient queries and avoid surprise costs.</p>"},{"location":"data-warehouse-performance/#usage-based-pricing","title":"Usage-based pricing","text":"<p>With most on-premise transactional warehouses, costs scale with the number of server instances you buy and run. These servers usually are always-on and power various applications with high availability. In a traditional transactional warehouse both compute power and storage are associated with the same logical machine.</p> <p>Cloud data warehouses typically have a different pricing model: they decouple storage and compute and charge based on your query usage. Snowflake charges based on the amount of compute resources needed to execute your queries. Google BigQuery charges based on the amount of data your queries scan. There are also costs associated with data storage, but those are usually small compared to compute. Though these two models are slightly different, they both lead to a similar take-home lesson: by being careful with how data are laid out and accessed, you can significantly reduce both execution time and cost for your cloud data warehouses.</p>"},{"location":"data-warehouse-performance/#data-layout","title":"Data layout","text":"<p>Most cloud data warehouses use columnar storage for their data. This means that data for each column of a table are stored sequentially in object storage (this is in contrast to transactional databases which usually store each row, or record, sequentially in storage). This BigQuery blog post goes into a bit more detail.</p> <p></p> <p>There are a number of advantages to using columnar storage for analytical workloads:</p> <ul> <li>You can read in columns separately from each other.     So if your query only needs to look at one column of a several-hundred column table,     it can do that without incurring the cost of loading and processing all of the other columns.</li> <li>Because the values in a column are located near each other in device storage,     it is much faster to read them all at once for analytical queries like aggregations or window functions.     In row-based storage, there is much more jumping around to different parts of memory.</li> <li>Having values of the same data type stored sequentially allows for much more efficient     serialization and compression of the data at rest.</li> </ul> <p>In addition to columnar storage, cloud data warehouses also usually divide tables row-wise into chunks called partitions. Different warehouses choose different sizing strategies for partitions, but they are typically from a few to a few hundred megabytes. Having separate logical partitions in a table allows the compute resources to process the partitions independently of each other in parallel. This massively parallel processing capability is a large part of what makes cloud data warehouses scalable. When designing your tables, you can often set partitioning strategies or clustering keys for the table. This tells the cloud data warehouse to store rows with similar values for those keys within the same partitions. A well-partitioned table can enable queries to only read from the partitions that it needs, and ignore the rest.</p>"},{"location":"data-warehouse-performance/#constructing-queries-for-cloud-data-warehouses","title":"Constructing queries for cloud data warehouses","text":"<p>With the above understanding of how cloud data warehouses store and process data, we can write down a set of recommendations for how to construct efficient queries for large tables stored within them:</p> <ul> <li>Only <code>SELECT</code> the columns you need.     Columnar storage allows you to ignore the columns you don't need,     and avoid the cost of reading it in. <code>SELECT *</code> can get expensive!</li> <li>If the table has a natural ordering, consider setting a clustering key.     For example, if the data in the table consists of events with an associated timestamp,     you might want to cluster according to that timestamp.     Then events with similar times would be stored near each other in the same or adjacent partitions,     and queries selecting for a particular date range would have to scan fewer partitions.</li> <li>If the table has a clustering key already set, try to filter based on that in your queries.     This can greatly reduce the amount of data you need to scan. The queries based on these filters     should be as simple as you can manage, complex predicates on clustered columns can make it     difficult for query optimizers to prune partitions.     You can tell if Snowflake has a clustering key set by inspecting the table definition for a <code>cluster by</code> clause.</li> <li>Filter early in complex queries, rather than at the end.    If you have complex, multi-stage queries, filtering down to the subset of interest at the outset    can avoid the need to process unnecessary data and then throw it away later in the query.</li> </ul> <p>Note</p> <p>For people coming from transactional databases, the considerations about clustering may seem reminiscent of indexes. Cloud data warehouses usually don't have traditional indexes, but clustering keys fill approximately the same role, tailored to the distributed compute model.</p>"},{"location":"data-warehouse-performance/#primary-keys-and-constraints","title":"Primary keys and constraints","text":"<p>A central feature of cloud data warehouses is that storage is separate from compute, and data can be processed in parallel by distributed compute resources that are only running (and thus only costing you money) while they are needed. The less communication that needs to happen between these distributed compute resources, the faster they can work. For this reason, most cloud data warehouses do not support primary keys, foreign keys, or other constraints.</p> <p>For example: if we have a foreign key constraint set on a table and insert a new record, we would have to scan every single row of the parent table to see if the referenced row exists and is unique. If the table is large and partitioned, this could mean spinning up a large amount of compute resources, just to insert a single row. So rather than supporting constraints with horrible performance characteristics, cloud data warehouses just don't do it. This can be surprising to some people, since they often still include the syntax for constraints for SQL standard compatibility (see the Snowflake docs on constraints).</p> <p>Note</p> <p>One exception to the above is <code>NOT NULL</code> constraints, which can be done cheaply since they don't require information from other tables or partitions to be enforced.</p>"},{"location":"data-warehouse-performance/#interactive-exercise","title":"Interactive Exercise","text":"<p>This exercise is intended to be done live with collaborators. It should read fine, but will be more impactful if we set up a lab setting!</p> <p>We'll be querying PeMS vehicle detector station data stored in Snowflake. The dataset in question consists of (at the time of this writing) 20+ years of detector data at thirty second resolution. It has a quarter of a trillion events and takes over 2 terabytes of storage.</p> <p>Because it is event data with a timestamp, we have set a clustering key for the event date on the column <code>SAMPLE_DATE</code>, so that all events with the same date are in the same (or adjacent) partitions.</p> <p>All queries for this exercise were run on an XL Snowflake warehouse.</p>"},{"location":"data-warehouse-performance/#initial-query","title":"Initial query","text":"<p>Suppose we want to know how many vehicles are counted in the first lane of each station for each calendar date in 2023. An initial query might look something like this:</p> <pre><code>SELECT\n    ID,\n    SAMPLE_DATE,\n    SUM(FLOW_1) AS FLOW_1,\n    SUM(FLOW_2) AS FLOW_2,\n    SUM(FLOW_3) AS FLOW_3,\n    SUM(FLOW_4) AS FLOW_4,\n    SUM(FLOW_5) AS FLOW_5,\n    SUM(FLOW_6) AS FLOW_6,\n    SUM(FLOW_7) AS FLOW_7,\n    SUM(FLOW_8) AS FLOW_8\nFROM RAW_PRD.CLEARINGHOUSE.STATION_RAW\nWHERE SAMPLE_DATE &gt; date_from_parts(2023, 1, 1)\nAND SAMPLE_DATE &lt; date_from_parts(2024, 1, 1)\nGROUP BY ID, SAMPLE_DATE\n</code></pre> <p>This query took about three minutes to run. Not too bad! But when we look at some statistics from the query profile, we start to see some problems:</p> <p></p> <p>Yikes! We can see from the \"Bytes scanned\" that this query scanned almost a terabyte of data, or about half of the overall dataset. Ideally, we should not scan the entire dataset to get some relatively simple statistics for a single year. We can also see that from the \"Profile Overview\", remote disk I/O takes about 86% of the execution time. This represents reads/writes to and from Snowflake's abstract cloud object storage. Since this is the bulk of the execution time, anything we can do to cut down on disk I/O will be worthwhile.</p> <p>Based on an XL warehouse's burn rate of about $1 per minute, this query costs about $3. If we were running this many times a day, it could easily cost thousands of dollars per year.</p>"},{"location":"data-warehouse-performance/#take-advantage-of-column-pruning","title":"Take advantage of column pruning","text":"<p>Let's see what we can do to cut down on the amount of data that is read by our query. Note that we are doing an aggregation on all eight lanes of traffic in the dataset, but our initial question only asked about the first lane of traffic. We are computing (and thus reading) more data than we actually need for the problem! Because data are stored in a columnar layout, it's easy to reduce the data scanned by just not selecting the columns we don't need.</p> <p>Let's just select the flow for the first lane:</p> <pre><code>SELECT\n    ID,\n    SAMPLE_DATE,\n    SUM(FLOW_1) AS FLOW\nFROM RAW_PRD.CLEARINGHOUSE.STATION_RAW\nWHERE SAMPLE_DATE &gt; date_from_parts(2023, 1, 1)\nAND SAMPLE_DATE &lt; date_from_parts(2024, 1, 1)\nGROUP BY ID, SAMPLE_DATE\n</code></pre> <p>This ran in under two minutes, and by inspecting the query profile, it's clear why: this query scanned a little over half the amount of data as the first one. By selecting the only the columns we wanted, we avoided loading a lot of unnecessary data!</p> <p></p> <p>Manually selecting all table columns or using <code>SELECT *</code> is often a bad idea in any database, but it can have particularly bad performance consequences for columnar data warehouses.</p>"},{"location":"data-warehouse-performance/#take-advantage-of-partition-pruning","title":"Take advantage of partition pruning","text":"<p>Note</p> <p>TL;DR: if a large table has a clustering key set, try to use it in filters, aggregations, and windowing operations. Try to keep predicates using that key as simple as possible to help out the query optimizer.</p> <p>There is another problem with the performance of the above queries. You can see that there are about two hundred thousand partitions in the entire dataset (\"Partitions total\"). And the number of partitions that the queries scanned (\"Partitions scanned\") was the same! That is to say, we needed to read through every single partition, even though we were only interested in the partitions corresponding to 2023.</p> <p>Because we set a clustering key for the <code>SAMPLE_DATE</code>, we should be able to efficiently select records for specific date ranges. However, query optimizers are not always as good at reasoning about partitions as we would like them to be. In this case, Snowflake is unable to reason about the result of the <code>date_from_parts()</code> function well enough to know what its result will be (even though it will result in a constant date!).</p> <p>Since Snowflake cannot figure out the result of the <code>date_from_parts()</code> call, it gives up and scans every partition to figure out if its date is satisfied by the filtering predicates. Inefficient queries scan every single partition. If, however, we write a hard-coded date string, Snowflake's query optimizer is able to figure out which partitions it needs to read. So the following query, which is logically equivalent, has much better performance characteristics</p> <pre><code>SELECT\n    ID,\n    SAMPLE_DATE,\n    SUM(FLOW_1) AS FLOW\nFROM RAW_PRD.CLEARINGHOUSE.STATION_RAW\nWHERE SAMPLE_DATE &gt; '2023-01-01'\nAND SAMPLE_DATE &lt; '2024-01-01'\nGROUP BY ID, SAMPLE_DATE\n</code></pre> <p>This runs in about seventeen seconds, and only scans a small fraction of the partitions in the dataset:</p> <p></p> <p>You'll also note that the total processing time is much more weighted towards actual data processing rather than I/O. This is a good thing: we want our compute processes to be working hard for us, rather than waiting idly for data to traverse the network while they cost money.</p> <p>The moral here is: partition pruning can drastically reduce the amount of data you need to process, but you sometimes need to be careful about query construction to have it behave properly. Pay attention to the results of the query profile if your queries are not having the performance you expect!</p>"},{"location":"data-warehouse-performance/#summary","title":"Summary","text":"<p>By using our knowledge about how cloud data warehouses work and how their data are laid out, we were able to take a query that took several minutes and cost a few dollars to one that took a fraction of the time and cost a few cents.</p> <p>It's a good idea to keep an eye on the query profiler to make sure your queries are performing as expected (especially for ones that are run frequently, or touch a lot of data).</p>"},{"location":"data-warehouse-performance/#references","title":"References","text":"<ul> <li>Snowflake documentation on clustering and micropartitions</li> <li>Snowflake documentation on query performance profiling</li> <li>Blog post on BigQuery's strategy for data layout and storage</li> <li>BigQuery partitioning guide</li> <li>BigQuery optimization guide (most tips apply more generally to CDWs)</li> </ul>"},{"location":"data-warehouse-performance/#glossary","title":"Glossary","text":"<p>Clustering key: A clustering key is a special column in a table which instructs the database to order     the records in the table according to that column. In cloud data warehouses which partition     their tables into chunks, records with the same value for a clustering key will be     stored in the same (or nearby) partitions. Foreign key: A foreign key is a special column in a table which refers to the primary key of another     table. Using foreign keys allows related tables to be linked, and they are often used in join operations.     Traditional transactional databases often have strict guarantees (or constraints) around foreign keys.     Cloud data warehouses typically do not respect those constraints. Partition: A partition is a chunk of rows in a database table that are stored contiguously in memory.     Cloud data warehouses commonly organize their tables into partitions of tens to hundreds of megabytes. Partition pruning: The process by which a query optimizer decides whether it needs to read a given     partition for a query. Primary key: A primary key is a special column in a table which serves as an identifier for a record.     In traditional transactional databases, they are usually required to be non-null and unique.     Cloud data warehouses typically do not respect those constraints.</p>"},{"location":"incremental/","title":"Incremental models","text":"<p>Since this project involves larger datasets, dbt incremental models are extremely important to save on time and compute costs. However, they are also more complex to reason about and manage.</p>"},{"location":"incremental/#utility-macros","title":"Utility macros","text":""},{"location":"incremental/#get_snowflake_refresh_warehouse","title":"<code>get_snowflake_refresh_warehouse()</code>","text":"<p>TODO</p>"},{"location":"incremental/#make_model_incremental","title":"<code>make_model_incremental()</code>","text":"<p>TODO</p>"},{"location":"incremental/#incremental-models-in-ci","title":"Incremental models in CI","text":"<p>TODO</p>"},{"location":"incremental/#full-refreshes","title":"Full refreshes","text":"<p>Note</p> <p>This involves changing data in production. It should be done with care!</p> <p>Any time you change the schema (i.e., the columns or data types) of an incremental model, this triggers the need to perform a \"full refresh\" of that model. This is because the incremental logic typically involves merging the new data with the old data. With a change in the schema, there can be empty columns and hard-to-debug data quality issues, or the merge will just fail. To avoid this, we need to rebuild the production incremental tables from scratch.</p> <p>Right now, full refreshes are a fairly manual process.</p>"},{"location":"incremental/#1-create-a-production-target","title":"1. Create a production target","text":"<p>First, you need to include a new <code>target</code> in your <code>~/.dbt/profiles.yml</code> file for the production environment. Here is an example of a <code>profiles.yml</code> with both a development and a production target:</p> <pre><code>caltrans_pems:\n  target: dev\n  outputs:\n    prd:\n      type: snowflake\n      account: ngb13288\n      user: ian.rose@innovation.ca.gov\n      authenticator: externalbrowser\n      role: TRANSFORMER_PRD\n      database: TRANSFORM_PRD\n      warehouse: TRANSFORMING_XS_PRD\n      schema: ANALYTICS\n      threads: 8\n    dev:\n      type: snowflake\n      account: ngb13288\n      user: ian.rose@innovation.ca.gov\n      authenticator: externalbrowser\n      role: TRANSFORMER_DEV\n      database: TRANSFORM_DEV\n      warehouse: TRANSFORMING_XS_DEV\n      schema: DBT_IROSE\n      threads: 8\n</code></pre>"},{"location":"incremental/#2-use-the-dbt-command-line-interface-to-perform-the-refresh","title":"2. Use the dbt command line interface to perform the refresh","text":"<p>We next need to run the changed model and all of its downstream dependencies in full refresh mode. Here is an example of the command line invocation:</p> <pre><code>dbt build --target prd --full-refresh --select /path/to/the/model.sql+\n</code></pre> <p>A few notes about this command:</p> <ol> <li>Because we are fully rebuilding large tables with this command, it might take some time.</li> <li><code>--target prd</code> selects the production environment as opposed to the default development one.     It should be used sparingly, since it changes the data in prod!</li> <li><code>--full-refresh</code> triggers the full refresh. Models that use the <code>get_snowflake_refresh_warehouse()</code>     macro will select a larger Snowflake virtual warehouse when this flag is selected,     since doing the full refresh typically involves processing much more data.</li> <li>We use the <code>+</code> selector to indicate that every downstream model of the one with the schema change     should also get rebuilt.</li> </ol>"},{"location":"setup/","title":"Repository setup","text":"<p>These are instructions for individual contributors to set up the repository locally.</p>"},{"location":"setup/#install-dependencies","title":"Install dependencies","text":"<p>We use <code>uv</code> to manage our Python virtual environments. If you have not yet installed it on your system, you can follow the instructions for it here. Most of the ODI team uses Homebrew to install the package. We do not recommend installing <code>uv</code> using <code>pip</code>: as a tool for managing Python environments, it makes sense for it to live outside of a particular Python distribution.</p>"},{"location":"setup/#2-install-python-dependencies","title":"2. Install Python dependencies","text":"<p>If you prefix your commands with <code>uv run</code> (e.g. <code>uv run dbt build</code>), then <code>uv</code> will automatically make sure that the appropriate dependencies are installed before invoking the command.</p> <p>However, if you want to explicitly ensure that all of the dependencies are installed in the virtual environment, run <pre><code>uv sync\n</code></pre> in the root of the repository.</p> <p>Once the dependencies are installed, you can also \"activate\" the virtual environment (similar to how conda virtual environments are activated) by running <pre><code>source .venv/bin/activate\n</code></pre> from the repository root. With the environment activated, you no longer have to prefix commands with <code>uv run</code>.</p> <p>Which approach to take is largely a matter of personal preference:</p> <ul> <li>Using the <code>uv run</code> prefix is more reliable, as dependencies are always resolved before executing.</li> <li>Using <code>source .venv/bin/activate</code> involves less typing.</li> </ul>"},{"location":"setup/#3-install-dbt-dependencies","title":"3. Install dbt dependencies","text":"<p>dbt comes with some core libraries, but others can be added on. This command installs those extra libraries that are needed for this repo. It will have to be re-run each time the repo starts relying on a new dbt library.</p> <p>To install dbt dependencies, open a terminal and enter:</p> <pre><code>uv run dbt deps --project-dir transform\n</code></pre>"},{"location":"setup/#configure-snowflake","title":"Configure Snowflake","text":"<p>In order to use Snowflake (as well as the terraform validators for the Snowflake configuration) you should set some default local environment variables in your environment. This will depend on your operating system and shell. For Linux and Mac OS systems, as well as users of Windows subsystem for Linux (WSL) it's often set in <code>~/.zshrc</code>, <code>~/.bashrc</code>, or <code>~/.bash_profile</code>.</p> <p>If you use zsh or bash, open your shell configuration file, and add the following lines:</p> <p>Default Transformer role</p> <pre><code># Legacy account identifier\nexport SNOWFLAKE_ACCOUNT=&lt;account-locator&gt;\n# The preferred account identifier is to use name of the account prefixed by its organization (e.g. myorg-account123)\n# Supporting snowflake documentation - https://docs.snowflake.com/en/user-guide/admin-account-identifier\nexport SNOWFLAKE_ACCOUNT=&lt;org_name&gt;-&lt;account_name&gt; # format is organization-account\nexport SNOWFLAKE_USER=&lt;your-username&gt;\nexport SNOWFLAKE_PASSWORD=&lt;your-password&gt;\nexport SNOWFLAKE_ROLE=TRANSFORMER_DEV\nexport SNOWFLAKE_WAREHOUSE=TRANSFORMING_XS_DEV\n</code></pre> <p>This will enable you to perform transforming activities which is needed for dbt. Open a new terminal and verify that the environment variables are set.</p> <p>Switch to Loader role</p> <pre><code># Legacy account identifier\nexport SNOWFLAKE_ACCOUNT=&lt;account-locator&gt;\n# The preferred account identifier is to use name of the account prefixed by its organization (e.g. myorg-account123)\n# Supporting snowflake documentation - https://docs.snowflake.com/en/user-guide/admin-account-identifier\nexport SNOWFLAKE_ACCOUNT=&lt;org_name&gt;-&lt;account_name&gt; # format is organization-account\nexport SNOWFLAKE_USER=&lt;your-username&gt;\nexport SNOWFLAKE_PASSWORD=&lt;your-password&gt;\nexport SNOWFLAKE_ROLE=LOADER_DEV\nexport SNOWFLAKE_WAREHOUSE=LOADING_XS_DEV\n</code></pre> <p>This will enable you to perform loading activities and is needed to which is needed for Airflow or Fivetran. Again, open a new terminal and verify that the environment variables are set.</p>"},{"location":"setup/#configure-dbt","title":"Configure dbt","text":"<p>The connection information for our data warehouses will, in general, live outside of this repository. This is because connection information is both user-specific usually sensitive, so should not be checked into version control. In order to run this project locally, you will need to provide this information in a YAML file located (by default) in <code>~/.dbt/profiles.yml</code> (where <code>~</code> indicates your home directory). To see the absolute path of where dbt is looking for your <code>profiles.yml</code>, run <code>dbt debug</code> and inspect the output.</p> <p>Instructions for writing a <code>profiles.yml</code> are documented here, as well as specific instructions for Snowflake.</p> <p>You can verify that your <code>profiles.yml</code> is configured properly by running</p> <pre><code>dbt debug\n</code></pre> <p>from the dbt project root directory (<code>transform</code>).</p>"},{"location":"setup/#snowflake-project","title":"Snowflake project","text":"<p>A minimal version of a <code>profiles.yml</code> for dbt development with is:</p> <pre><code>caltrans_pems:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: &lt;account-locator&gt;\n      user: &lt;your-username&gt;\n      password: &lt;your-password&gt;\n      authenticator: username_password_mfa\n      role: TRANSFORMER_DEV\n      database: TRANSFORM_DEV\n      warehouse: TRANSFORMING_XS_DEV\n      schema: DBT_&lt;your-name&gt;   # Test schema for development\n      threads: 4\n</code></pre>"},{"location":"setup/#installing-pre-commit-hooks","title":"Installing <code>pre-commit</code> hooks","text":"<p>This project uses pre-commit to lint, format, and generally enforce code quality. These checks are run on every commit, as well as in CI.</p> <p>To set up your pre-commit environment locally run</p> <pre><code>pre-commit install\n</code></pre> <p>The next time you make a commit, the pre-commit hooks will run on the contents of your commit (the first time may be a bit slow as there is some additional setup).</p> <p>You can verify that the pre-commit hooks are working properly by running</p> <p><pre><code>pre-commit run --all-files\n</code></pre> to test every file in the repository against the checks.</p> <p>Some of the checks lint our dbt models, so having the dbt project configured is a requirement to run them, even if you don't intend to use those packages.</p>"},{"location":"writing-documentation/","title":"Writing Documentation","text":"<p>Documentation for this project is built using mkdocs with the material theme and hosted using GitHub Pages. The documentation source files are in the <code>docs/</code> directory and are authored using markdown.</p>"},{"location":"writing-documentation/#local-development","title":"Local Development","text":"<p>To write documentation for this project, make sure that the repository is set up. You should then be able to start a local server for the docs:</p> <pre><code>mkdocs serve\n</code></pre> <p>Then open a web browser to http://localhost:8000 to view the built docs. Any edits you make to the markdown sources should be automatically picked up, and the page should automatically rebuild and refresh.</p>"},{"location":"writing-documentation/#deployment","title":"Deployment","text":"<p>Deployment of the docs for this repository is done automatically upon merging to <code>main</code> using the <code>docs</code> GitHub Action.</p> <p>Built documentation is pushed to the <code>gh-pages</code> branch of the repository, and can be viewed by navigating to the GitHub pages URL for the project.</p>"},{"location":"data/G-factor%20speed/","title":"G-Factor Speed","text":"<p>The PeMS system possesses the ability to compute speed for sensors that don't report speed, like single loop detectors. In fact, even many double loop detectors, which often fail to report speed, depend on estimations. Single loop detectos are prevalent for traffic measurement like occupancy and flow but do not directly measure speed. By establishing a relationship between vehicle count, occupancy, and vehicle length, this model proposes a method to estimate speed indirectly.</p>"},{"location":"data/G-factor%20speed/#methodology","title":"Methodology","text":"<ul> <li> <p>Statement: Single loop detectors estimates vehicle speed by dynamically adjusting the assumed vehicle length based on observed traffic conditions, overcoming the limitations of fixed-length assumptions commonly used in other models. The estimated vehicle length is called g-factor.</p> </li> <li> <p>Procedures: The steps for estimation of speed from is available via PeMS Documentation, which we won't duplicate the documentation here.</p> </li> </ul>"},{"location":"data/G-factor%20speed/#stl-decomposition-for-comparing-speed-datasets","title":"STL Decomposition for Comparing Speed Datasets","text":""},{"location":"data/G-factor%20speed/#why-speed-qc","title":"Why Speed QC","text":"<ul> <li>Ambiguity: We noted that even though the existing PeMS system claimed to follow the procedures of the original paper, they have made significant changes to smooth the speed calculation without additional detailed documentations that we can follow. It turns out difficult for us to generate the exact same logic to estimate the speed.</li> <li>Availability: Through investigations, it seems hard to obtain the datasets of other components, including g-factor and p-factor. They are key parameters to implement a kalman filter for speed smoothing.</li> <li>Accuracy: Speed is a fundamental metric in PeMS that directly influences the assessment of downstream performance metrics, such as bottleneck identification, congestion levels and VHT. Before performance metrics QC, it is essential to guarantee the accuracy of speed calculation as a basis.</li> <li>Quantity: Visual assessments are insufficient to measure the difference. To accurately identify and measure these variations and adjusted parameters at the detector level, it is essential to adopt quantitative methods.</li> </ul>"},{"location":"data/G-factor%20speed/#why-use-stl-algorithm","title":"Why Use STL Algorithm?","text":"<p>Seasonal-Trend Decomposition using Loess (STL) is a robust method for decomposing a time series into three additive components:</p> <ul> <li>Trend: The long-term progression of the series.</li> <li>Seasonal: The repeating short-term cycle in the series.</li> <li>Residual: The remaining variation after removing trend and seasonal components.</li> </ul> <p>A speed time series $Y_t$ can be expressed as:</p> <pre><code>Y_t = T_t + S_t + R_t\n</code></pre> <ul> <li>$T_t$: Trend component.</li> <li>$S_t$: Seasonal component.</li> <li>$R_t$: Residual component.</li> </ul> <pre><code>def decompose_lane(data, station_lane, dataset_name):\n    # Filter data for the lane\n    lane_data = data[data['station_lane'] == station_lane].copy()\n    lane_data.set_index('timestamp', inplace=True)\n    lane_data.sort_index(inplace=True)\n\n    # Ensure sufficient data points for STL\n    if len(lane_data) &lt; 288: # Drop lane data if the sample size is less in a day\n        return None\n\n    # Set the seasonal period\n    seasonal_period = 288\n\n    # Apply STL decomposition\n    stl = STL(lane_data['speed'], period=seasonal_period, robust=True).fit()\n    lane_data['trend'] = stl.trend\n    lane_data['seasonal'] = stl.seasonal\n    lane_data['resid'] = stl.resid\n\n    # Add dataset identifier\n    lane_data['dataset'] = dataset_name\n\n    return lane_data\n</code></pre>"},{"location":"data/G-factor%20speed/#experimental-study","title":"Experimental Study","text":"<p>Goal: Compare speed calculations from the existing PeMS system and multiple versions of the modernized PeMS system to determine which modernized version best matches the existing system. The adjusting parameters including:</p> <ul> <li>[x] Baseline: Fixed vehicle length vs dynamic vehicle length (g-factor or not)</li> <li>[x] Spatial: G-factor aggregation: detector, station, corridor</li> <li>[x] Temporal: Moving average smoothing length: 4, 12</li> </ul> Variation Baseline Spatial Temporal 1 \u2714\ufe0f 2 detector 3 station 4 corridor 5 detector 4 6 station 4 7 corridor 4 8 detector 12 9 station 12 10 corridor 12"},{"location":"data/G-factor%20speed/#data-preparation-sr-91","title":"Data Preparation: SR 91","text":""},{"location":"data/G-factor%20speed/#datasets-information","title":"Datasets information:","text":"<ol> <li>Time Window: <code>10/4/2024 - 10/10/2024</code></li> <li>Temporal Aggregation Level: <code>5-minute</code></li> <li>Spatial Aggregation Level: Detector level (<code>803</code> unique stations containing different numbers of lanes).</li> <li>Detector Status: Choose only detectors which are diagnosed as <code>good</code>. Specifically, for existing PeMS speed, we only choose data if there are two consecutive days reporting good to get rid of imputation data.</li> <li>Sample Size: Around <code>700000</code> records for each variation.</li> <li>Seasonality: <code>288</code> Assume the speed has a daily pattern (12*24 = 288).</li> </ol>"},{"location":"data/G-factor%20speed/#evaluation-metrics","title":"Evaluation Metrics:","text":""},{"location":"data/G-factor%20speed/#statistical-tests","title":"Statistical Tests","text":"<p>For each detector with 3 components (<code>trend</code>, <code>seasonal</code>, <code>resid</code>), we perform independent Two-sample T-Test:</p> <ul> <li>$H_0$: The means of components in existing speed are equal to the ones in modernized speed.</li> <li>$H_a$: Their means are different.</li> </ul> <pre><code>def compare_components(lane, component, data_old, data_modern):\n\n    data_old_comp = data_old.loc[data_old['station_lane'] == lane, component].copy()\n    data_modern_comp = data_modern.loc[data_modern['station_lane'] == lane, component].copy()\n\n    data_old_comp = data_old_comp.dropna()\n    data_modern_comp = data_modern_comp.dropna()\n\n    # Perform t-test\n    t_stat, p_value = ttest_ind(\n        data_old_comp,\n        data_modern_comp,\n        equal_var=False\n    )\n    return t_stat, p_value\n</code></pre>"},{"location":"data/G-factor%20speed/#summary-tables","title":"Summary Tables","text":"<ol> <li>A component is significantly different if $p$ &lt; 0.05.</li> <li>Count the number of detectors with significant differences for each component and calculate the percentage relative to the total number of detectors analyzed.</li> <li>Compare all modernized PeMS speed datasets to the existing PeMS dataset.</li> <li>Identify the dataset with the lowest percetange of significant differences.</li> </ol> <pre><code># Summarize significant differences\nsignificant_differences = component_tests_df[component_tests_df['p_value'] &lt; 0.05]\ntotal_station_lanes = component_tests_df['station_lane'].nunique()\nsignificant_counts = significant_differences.groupby('component')['station_lane'].nunique().reset_index()\nsignificant_counts.rename(columns={'station_lane': 'significant_station_lanes'}, inplace=True)\ntotal_counts = component_tests_df.groupby('component')['station_lane'].nunique().reset_index()\ntotal_counts.rename(columns={'station_lane': 'total_station_lanes'}, inplace=True)\nsummary_df = pd.merge(total_counts, significant_counts, on='component', how='left')\nsummary_df['significant_station_lanes'].fillna(0, inplace=True)\nsummary_df['percentage'] = (summary_df['significant_station_lanes'] / summary_df['total_station_lanes']) * 100\n</code></pre>"},{"location":"data/G-factor%20speed/#comparison-of-significant-differences-across-datasets","title":"Comparison of Significant Differences Across Datasets (%)","text":"Model seasonal resid trend 1 2.24 49.30 98.32 2 1.68 62.75 98.88 3 1.68 61.06 98.60 4 1.68 62.75 98.88 5 0.84 55.74 96.64 6 1.68 59.10 98.60 7 1.68 61.06 98.88 8 1.12 51.54 95.51 9 1.68 57.14 97.48 10 2.52 59.10 98.60 <p>The results show that all models handle seasonal variations well, with only small differences between datasets. Models 5 and 8 have the least discrepancies. However, there are significant differences in the residuals and trends: residuals are about 50%, and trends are over 95%. The residuals remain because not all seasonal and trend patterns are removed by the STL process, which is expected due to the high volatility of the 5-minute speed dataset. This means the residuals still remain partial trend and seasonality patterns, therefore not following simple white noise patterns. In the future, we might use more complex models to better capture the patterns in speed data. As for the trends through a sample visualization, it is highly nonlinear. Using a t-test to compare them isn\u2019t suitable because time series data don\u2019t meet the normality condition required for t-tests. We need to find other ways to analyze these non-linear patterns.</p> <p></p>"},{"location":"data/G-factor%20speed/#mae-rmse-and-mape","title":"MAE, RMSE, and MAPE","text":"<p>One way is to quantitatively measure the difference between two models using error metrics for trend component to compare performance among models.</p> <ul> <li>Mean Absolute Error: MAE measures the average error magnitude that each error contributes with the same weight to the total average.</li> <li>Root Mean Squared Error: RMSE is sensitive to large errors.</li> <li>Mean Absolute Percentage Error: MAPE express error as a percentage of the actual values, which is useful when dealing with relative errors and percentages.</li> </ul> <pre><code>def compute_error_metrics(lane, data_old, data_modern, component='trend'):\n    # Extract components for the lane\n    comp_old = data_old.loc[data_old['station_lane'] == lane, component].copy()\n    comp_modern = data_modern.loc[data_modern['station_lane'] == lane, component].copy()\n\n\n    merged_data = pd.merge(\n        comp_old.reset_index(),\n        comp_modern.reset_index(),\n        on='timestamp',\n        how='inner',\n        suffixes=('_old', '_modern')\n    )\n\n    # Drop NaNs\n    merged_data.dropna(subset=[f'{component}_old', f'{component}_modern'], inplace=True)\n\n    # Check if data is sufficient\n    if len(merged_data) == 0:\n        return None, None, None\n\n    # Compute error metrics\n    mae = np.mean(np.abs(merged_data[f'{component}_old'] - merged_data[f'{component}_modern']))\n    rmse = np.sqrt(np.mean((merged_data[f'{component}_old'] - merged_data[f'{component}_modern']) ** 2))\n    with np.errstate(divide='ignore', invalid='ignore'):\n        mape = np.mean(np.abs((merged_data[f'{component}_old'] - merged_data[f'{component}_modern']) / merged_data[f'{component}_old'])) * 100\n    return mae, rmse, mape\n\n    # Align data by timestamps\n    merged_data = pd.merge(\n        comp_old.reset_index(),\n        comp_modern.reset_index(),\n        on='timestamp',\n        how='inner',\n        suffixes=('_old', '_modern')\n    )\n\n    # Drop NaNs\n    merged_data.dropna(subset=[f'{component}_old', f'{component}_modern'], inplace=True)\n\n    # Check if data is sufficient\n    if len(merged_data) &lt; 10:\n        return None, None\n</code></pre>"},{"location":"data/G-factor%20speed/#comparison-of-error-metrics-for-trend-comparison","title":"Comparison of Error Metrics for Trend Comparison","text":"Model MAE RMSE MAPE 1 9.83 10.22 16.38 2 8.50 8.81 14.27 3 4.43 4.61 7.24 4 8.50 8.81 14.27 5 3.17 3.50 5.32 6 4.33 4.63 7.27 7 8.49 8.80 14.25 8 3.21 3.55 5.40 9 4.37 4.67 7.32 10 8.50 8.81 14.25 <p>According to the error metrics, model 5 and 8 still outperforms other models in terms of trend. We choose model 8 as the final model for g-factor speed calculation.</p>"},{"location":"data/bottlenecks/","title":"Bottlenecks","text":"<p>The PeMS system runs a bottleneck identification algorithm every day. The original algorithm was presented in, \"Systematic Identification of Freeway Bottlenecks,\" by Chen, C., Skabardonis, A., Varaiya, P. (2003). Transportation Research Board, 2004.</p> <p>This model identifies bottlenecks at every station. A bottleneck is defined by the following conditions:</p> <ol> <li>There is a drop in speed of at least 20 mph between stations during the same time</li> <li>The speed at the current station (downstream station) is less than 40 mph.</li> <li>The stations are less than 3 miles apart.</li> <li>The speed drop persists for at least 5 out of any 7 contiguous 5-minute data points.</li> </ol> <p>If all of these conditions are met then we declare that there is a bottleneck at this location that has been activated for all of the seven 5-minute time points. We perform this analysis for each of three time periods:</p> <ul> <li>AM shift (5am - 10am)</li> <li>Noon shift (10am - 3pm)</li> <li>PM shift (3pm - 8pm)</li> </ul> <p>For each location where a bottleneck is activated we compute the following:</p> <ol> <li>Duration - This is how long the bottleneck was active during that particular shift on that day.</li> <li>Spatial Extent - For each 5-minute period when the bottleneck was active we see how far upstream the bottleneck extends. For this, at each time point, we find the farthest upstream detector with speed less than 40 mph and use that location as the extent during that 5-minute period. To get a single extent for the entire bottleneck, we take the median of the extents for each 5-minute period.</li> <li>Delay - The delay is simply the sum of the individual PeMS segment delays for the entire duration and spatial extent of the bottleneck. The delay is calculated with respect to a threshold speed of 60 mph.</li> </ol> <p>Here we can see a GIS representation of bottleneck and read additional details of what this map represents.</p>"},{"location":"data/bottlenecks/#bottleneck-direction-considerations","title":"Bottleneck direction considerations","text":"<p>North/East Absolute postmile increases going north and east. This means when the direction of the freeway for a station is north or east, the \"upstream\" station has a smaller postmile, and we need to lag our calculations. </p> <p>South or West Absolute postmile decreases going south and west. This means when the direction of the freeway for a station is south or west, the \"upstream\" station has a larger postmile, and we need to lead our calculations. </p> <p>Bottlenecks are analyzed across all lanes so any variables we use in developing bottleneck-related calculations should also be across all lanes at a given station. This also means we do not partition the data by lane.</p> <p>Additional information about bottlenecks from FHWA can be found at the following links. These details do not impact current bottleneck analysis but can be useful in future discussions: https://ops.fhwa.dot.gov/bn/lbr.htm and https://ops.fhwa.dot.gov/bn/bnchart.htm</p> <p>Below we can see an example of five instances (highlighted in green) of when the four conditions mentioned above are met out of a rolling window of 7. </p>"},{"location":"data/bottlenecks/#our-current-implementation-of-bottlenecks-versus-systematic-identification-of-freeway-bottlenecks","title":"Our current implementation of Bottlenecks versus Systematic Identification of Freeway Bottlenecks","text":"<p>The diagram below illustrates the logic in the \"Systematic Identification of Freeway Bottlenecks\" paper linked to above and also states how our implementation is a reversal. We are currently in agreement that while our implementation is different, it is also easier to understand. To match our logic with what is in the paper we'd simply have to do sign reversal in various places in our model.</p> <p></p>"},{"location":"data/data-relay/","title":"Data Relay Documentation","text":""},{"location":"data/data-relay/#overview","title":"Overview","text":""},{"location":"data/data-relay/#backgrounds","title":"Backgrounds","text":""},{"location":"data/data-relay/#why-we-need-data-relay","title":"Why we need Data Relay","text":"<ul> <li>Caltrans IT infrastructure collects the road side detector signals and transforms them within the Caltrans network.</li> <li>Data Relay moves PeMS data from internal Caltrans networks to the cloud (Snowflake).</li> <li>Snowflake employs elastic computing resources to transform the data within the data warehouse, which are then consumed by PeMS users.</li> </ul> <p>Therefore, Data Relay performs the critical link between the Caltrans infrastructure and the Snowflake.</p> <p>The data sources needed for the PeMS system include: * The <code>VDS30SEC</code> table. * The sensor device configuration tables.</p>"},{"location":"data/data-relay/#how-data-relay-works","title":"How Data Relay works","text":"<ul> <li>On a regular basis, Data Relay service hosted in Caltrans internal network pulls PeMS upstream data and batch them for uploading to Snowflake.</li> <li>The current upstream for Data Relay is the legacy PeMS Oracle database that serves the legacy PeMS website.</li> </ul>"},{"location":"data/data-relay/#general-system-design-principle","title":"General System Design Principle","text":"<ul> <li>Data Relay is the only component in the entire PeMS modernization project that operates within Caltrans internal networks, thus there are upfront limitations we need to consider when we build Data Relay service a) Caltrans IT team will not provide support on operating the Data Relay on a routine basis, though they will address the incidents when the hosting machines go down. b) The legacy PeMS Oracle database was intended to only serve the legacy PeMS website frontend; supporting an additional data downstream service that powers an entire data system was not in its scope. So does the PeMS Oracle databases' internal networking, which was never purposed for directly transfer data to the public internet. c) Cloud system's data ingestion needs to work with large batch of data in compressed form; therefore, the data relay should output that kind of data.</li> </ul> <p>With those constraints set forth, there are extensive studies and experimentation done before we settled down our final architecture. Here is our high-level journey: * We considered using Airflow, which provides a powerful tooling support for administering batch-based data pipeline workloads with reliability. The problem, however, is the constraint a), which would force us to include managing an Airflow platform as part of the PeMS modernization project scope, which would increase our cost. * We then tried using one machine running python script with crontab as scheduling. It does not come with a heavy-weight of managing an Airflow system, and it may handle any complex data transforming operations with python libraries, but due to the constraint b above, such python script cannot simultaneously talking to Oracle and Snowflake at once. * We then tried Kafka. Kafka as a distributed pub-sub services naturally fit into Data Relay in that it offers Task Queue management out-of-the-box, and it orchestrates the machines in and out of the PeMS Oracle networks together (as aforementioned in constraint b, only using one machine for Data Relay for pulling Oracle and pushing to Snowflake is not possible). * In spite of Kafka's benefits, it nevertheless shares the same drawback as Airflow in that, managing a Kafka cluster has to be part of the project scope, which increases the cost. Fortunately, Kafka's administering cost is significantly lower than Airflow, in that it has low dependency, and machines are identical setup, unlike Airflow, which has to distinguish different roles such as scheduler - worker - database. * In practice, Kafka data is consumed using pub-sub way, which may incur some minor inconveniences to certain operation such as deduplicating such as ensuring configuration tables' elements are deduplicated before sending to Snowflake. Therefore, we scope Kafka to be focused on the VDS30SEC table relaying, which accounts for 99.9% of the daily new data.</p> <p>In sum, we adopted Kafka for our Data Relay service's backbone, and this design decision is the foundation for the architecture design we will introduce in the following sections.</p>"},{"location":"data/data-relay/#whole-system-components-in-a-nutshell","title":"Whole System Components In a nutshell","text":"<ol> <li> <p>PeMS Oracle Database (DB96/DWO): The source of the data that needs to be pulled and relayed.</p> </li> <li> <p>Servers</p> <ul> <li> <p>svgcmdl01 (Puller Side):</p> <ul> <li>Gateway: <code>/etc/systemd/system/oracle_puller.service</code></li> <li>Configuration: <code>/etc/systemd/system/oracle_puller.service.sh</code></li> <li>Script: <code>/data/projects/crawler/oracle_puller_daemon.py</code></li> <li>Data Source: Oracle Database (DB96/DWO)</li> <li>Process: Continuously pulls data from the Oracle databases based on tasks generated by the task queue on svgcmdl02. The pulled data is then sent to the Kafka service.</li> </ul> </li> <li> <p>svgcmdl02 (Uploader Side):</p> <ul> <li>Task Queue Generators:<ul> <li>Cron Jobs: <code>append_to_crawl_queue.py</code> (runs every 3 hours) generate tasks for the puller on svgcmdl01, scheduling SQL queries to be executed periodically.</li> </ul> </li> <li>Data Uploaders:<ul> <li>Script: <code>aws_upload_daemon_script.sh</code> (runs every 6 hours) processes the pulled data and uploads it to AWS S3/Snowflake.</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Kafka Service</p> <ul> <li>Purpose: Centralized service to manage data flow between the task producers and consumers.</li> <li>Topics:<ul> <li>Task Queue Topic: <code>oracle_crawl_queue_topic</code> (where tasks are published for the puller to execute)</li> <li>Data Buffer Topics: <code>D3.VDS30SEC</code>, <code>D4.VDS30SEC</code>, ..., <code>D12.VDS30SEC</code> (where pulled data is buffered before being consumed by the uploader)</li> </ul> </li> </ul> </li> <li> <p>Data Flow</p> <ul> <li>Task Generation: Cron jobs on svgcmdl02 generate tasks every few hours and send them to Kafka.</li> <li>Task Execution: svgcmdl01 pulls tasks from Kafka, executes the tasks to retrieve data from the Oracle database, and then sends the data back to Kafka topics.</li> <li>Data Upload: The data is consumed from Kafka topics by svgcmdl02 and uploaded to AWS S3/Snowflake at scheduled intervals.</li> </ul> </li> <li> <p>Final Destination:</p> <ul> <li>AWS/Snowflake: The final destination for the data after it has been pulled, processed, and uploaded.</li> </ul> </li> </ol>"},{"location":"data/data-relay/#diagrams","title":"Diagrams","text":""},{"location":"data/data-relay/#key-components","title":"Key Components","text":"<pre><code>                                +-------------------+\n                                | PeMS Oracle       |\n                                | Database          |\n                                | DB96 / DWO        |\n                                +-------------------+\n                                         |\n                                         v\n                +-------------------------------------------------+\n                |           Server svgcmdl01 (Puller Side)        |\n                |                                                 |\n                | 1. Oracle Puller Daemon                         |\n                |    - Gateway:                   |\n        |   /etc/systemd/system/oracle_puller.service |\n                |    - Executes: oracle_puller_daemon.py          |\n                |    - Continuously pulls data from DB96/DWO      |\n                |                                                 |\n                | 2. Task Generation (via Kafka)                  |\n                |    - Task:                      |\n        |   SQL Queries from cron jobs on svgcmdl02   |\n                |    - Kafka Topic: oracle_crawl_queue_topic      |\n                |    - Puller consumes these tasks and pulls data |\n                |                                                 |\n                +-------------------------------------------------+\n                                         |\n                                         v\n                +-------------------------------------------------+\n                |           Server svgcmdl02 (Uploader Side)      |\n                |                                                 |\n                | 1. Cron Jobs                                    |\n                |    - Generate tasks for oracle_puller           |\n                |    - Example: db96 task every 3 hours           |\n                |                                                 |\n                | 2. Data Upload                                  |\n                |    - Upload pulled data to AWS/Snowflake        |\n                |    - Example: db96 data upload every 6 hours    |\n                |                                                 |\n                +-------------------------------------------------+\n                                         |\n                                         v\n                +-------------------------------------------------+\n                |           AWS / Snowflake                       |\n                |                                                 |\n                | 1. Stores the data uploaded from svgcmdl02      |\n                |    - Transformed DB96 / DWO tables              |\n                |                                                 |\n                +-------------------------------------------------+\n</code></pre>"},{"location":"data/data-relay/#data-flow","title":"Data Flow","text":""},{"location":"data/data-relay/#-svgcmdl01-kafka-service-svgcmdl02-puller-side-uploader-side-gateway-task-queue-topic-cron-jobs-configuration-data-buffer-data-uploaders-script-topics-data-source-data-pulled-by-oracle_puller-and-sent-to-kafka-data-consumed-by-aws_upload_daemon_-scriptsh-data-sent-to-kafka-data-uploaded-to-aws-s3snowflake","title":"<pre><code>+-------------------+       +-------------------+       +-------------------+\n|                   |       |                   |       |                   |\n|  svgcmdl01        |       |  Kafka Service    |       |  svgcmdl02        |\n|  (Puller Side)    |       |                   |       |  (Uploader Side)  |\n|                   |       |                   |       |                   |\n| - Gateway         |       | - Task Queue Topic|       | - Cron Jobs       |\n| - Configuration   |       | - Data Buffer     |       | - Data Uploaders  |\n| - Script          |       |   Topics          |       |                   |\n| - Data Source     |       |                   |       |                   |\n|                   |       |                   |       |                   |\n+-------------------+       +-------------------+       +-------------------+\n        |                           |                          |\n        |  Data pulled by           |                          |\n        |  `oracle_puller`          |                          |\n        |  and sent to Kafka        |                          |\n        +--------------------------&gt;|                          |\n        |                           |                          |\n        +---------------------------| ------------------------&gt;|\n        |                           |  Data consumed by        |\n        |                           | `aws_upload_daemon_      |\n        |                           |             script.sh`   |\n        |                           |                          |\n        +-------------------------- | ------------------------&gt;|\n            Data sent to Kafka      Data uploaded to AWS S3/Snowflake\n</code></pre>","text":""},{"location":"data/data-relay/#workflow-from-data-source-to-final-destination","title":"Workflow from Data Source to Final Destination","text":"<p>From: PeMS Oracle Database</p> <ul> <li>DB96</li> <li>DWO</li> </ul> <pre><code>ssh jupyter@svgcmdl01.dot.ca.gov\npassword:\n</code></pre> <p>To\uff1a Snowflake</p> <ul> <li>AWS S3</li> </ul> <pre><code># account 1\njupyter@svgcmdl02.dot.ca.gov\n# account 2\ns159123@svgcmdl02.dot.ca.gov\npassword:\n</code></pre>"},{"location":"data/data-relay/#puller-side-server-svgcmdl01","title":"Puller Side: <code>Server svgcmdl01</code>","text":""},{"location":"data/data-relay/#entry-pointgateway-of-the-program","title":"Entry point/Gateway of the program","text":"<p>system file automatically run by certain system software</p> <ul> <li>Gateway file: <code>Oracle_puller.service</code></li> </ul> <pre><code>sudo ls /etc/systemd/system\n# File structure\nGateway:\n\u251c\u2500\u2500 /etc/systemd/system\n\u2502   \u251c\u2500\u2500 oracle_puller.service\n\u2502   \u2514\u2500\u2500 oracle_puller.service.sh\n</code></pre> <ul> <li>Gateway Contents <code>Oracle_puller.service</code></li> </ul> <pre><code>sudo ls /etc/systemd/system/oracle_puller.service\n        # File contents\n        # ----------------------------------------------------\n        [Unit]\n        Description=Oracle Puller Daemon\n        After=network.target\n        [Service]\n        ExecStart=/etc/systemd/system/oracle_puller.service.sh\n        WorkingDirectory=/data/projects/crawler\n        Restart=always\n        User=jupyter\n\n        [Install]\n        WantedBy=multi-user.target\n        # ----------------------------------------------------\n</code></pre> <ul> <li>Gateway Configuration: <code>oracle_puller.service.sh</code></li> </ul> <pre><code>/etc/systemd/system/oracle_puller.service.sh\n        # File contents\n        # ----------------------------------------------------\n        #!/user/bin/bashrc\n        source /home/jupyter/.bashrc\n        cd /data/projects/crawler  # working folder\n        python3.9 oracle_puller_daemon.py # execute the program\n        # ----------------------------------------------------\n</code></pre> <ul> <li>Puller file structure</li> </ul> <pre><code>\u251c\u2500\u2500 /data/projects/crawler\n\u2502   \u2514\u2500\u2500 oracle_puller_daemon.py\n        # running continuously to pull data from Oracle\n</code></pre>"},{"location":"data/data-relay/#operations-on-the-program","title":"Operations on the program","text":"<ul> <li>check status of the system control file: <code>oracle_puller.service</code></li> </ul> <pre><code>sudo systemctl status oracle_puller\n    # key info\n    oracle_puller.service - Oracle Puller Daemon\n    CGroup: /system.slice/oracle_puller.service\n    \u2502   \u251c\u2500\u2500 /user/bin/bash /etc/systemd/system/oracle_puller.service.sh\n    \u2502   \u2514\u2500\u2500 python3.9 oracle_puller_daemon.py\n</code></pre> <ul> <li>Operations : Stop and Restart</li> </ul> <pre><code>sudo systemctl stop oracle_puller\nsudo systemctl restart oracle_puller\n</code></pre> <ul> <li>For new service, redefine the similar contents like the <code>oracle_puller.service</code></li> </ul> <pre><code>sudo vi /etc/systemd/system/oracle_puller.service\n</code></pre>"},{"location":"data/data-relay/#uploader-side-server-svgcmdl02","title":"Uploader Side: <code>Server svgcmdl02</code>","text":""},{"location":"data/data-relay/#cron-jobs-overview","title":"Cron Jobs  - Overview","text":"<ul> <li>Note: For the crontab , use the s159123 account</li> </ul> <pre><code>crontab -e\n# Service health checking, done at weekdays morning 730am\n34 7 * * 1-5 /home/s159123/check_services.sh\n\u00a0\n# Every hour, clean the json files under tmp that are 4 hours old\n0 * * * * find /tmp -name \"*.json\" -type f -mmin +240 -exec rm {} \\; 2&gt;/dev/null\n# ======================================== Task Queue Generator ==================================================================================================\n# DWO config data upload daily (upload config data to AWS)\n# db96 (generate the task queues for oracle_puller)\n0 2,5,8,11,14,17,20,23 * * * cd /home/s159123/gitrepo/dataop/agents/agent_oracle_puller_queue &amp;&amp; python3 append_to_crawl_queue.py --append_to_queue --githash '3110c63' --day_partition_number 8\n    # explanation -------------------------------------------------------------------\n        # 0: minutes\n        # 2,5,8,11,14,17,20,23: hours, 8 times in a day for the crontab job to\n                                                                   # launch the following script\n        # * * *: every day; day, month, year\n        # cd /home/s159123/gitrepo/dataop/agents/agent_oracle_puller_queue: go to the working folder\n        # python3 append_to_crawl_queue.py --append_to_queue --githash '3110c63' --day_partition_number 8\n    # purpose:\n        # sending the controlling commands to pull the DB96 every 3 hours\n    #---------------------------------------------------------------------------------\n\n# DWO config crawl task daily\n0 19 * * * cd /nfsdata/dataop/uploader &amp;&amp; python3.9 append_to_crawl_queue_config.py --githash 'ccccccc' --append_to_queue\n# ========================================Data Uploader ==================================================================================================\n# DWO config data upload daily (upload config data to AWS)\n0 23 * * * sh /nfsdata/dataop/uploader/aws_upload_daemon_script.sh \"CONTROLLER_CONFIG,CONTROLLER_CONFIG_LOG,DETECTOR_CONFIG,DETECTOR_CONFIG_LOG,STATION_CONFIG,STATION_CONFIG_LOG\"\n\u00a0\n# Cron scheduling the aws uploader, every 6 hours. (consume the data pulled from the oracle_puller, upload PeMS data to AWS)\n0 0,6,12,18 * * * sh /nfsdata/dataop/uploader/aws_upload_daemon_script.sh \"D3.VDS30SEC,D4.VDS30SEC,D5.VDS30SEC,D6.VDS30SEC,D7.VDS30SEC,D8.VDS30SEC,D10.VDS30SEC,D11.VDS30SEC,D12.VDS30SEC\"\n\u00a0\n\u00a0\n# 0 23 * * * sh /nfsdata/dataop/uploader/aws_upload_daemon_script.sh \"D3.VDS30SEC,D4.VDS30SEC,D5.VDS30SEC,D6.VDS30SEC,D7.VDS30SEC,D8.VDS30SEC,D10.VDS30SEC,D11.VDS30SEC,D12.VDS30SEC,CONTROLLER_CONFIG,CONTROLLER_CONFIG_LOG,DETECTOR_CONFIG,DETECTOR_CONFIG_LOG,STATION_CONFIG,STATION_CONFIG_LOG\"\n</code></pre>"},{"location":"data/data-relay/#task-queue-generators-on-svgcmdl02-d2","title":"Task Queue Generators on svgcmdl02 (D2)","text":"<p>Result/Output</p> <ul> <li>Crawl Queue: [task1, task2, task3, ...], [task_n, task_n+1, ...]</li> </ul> <p>Relation with <code>oracle_puller</code> :</p> <ul> <li><code>oracle_puller</code>: task (SQL statement) executor/consumer (svgcmdl01)</li> <li><code>crontab controller</code>: task (SQL statement)  generator/ producer (svgcmdl02)</li> </ul>"},{"location":"data/data-relay/#db96-task-generator-every-3-hours","title":"db96 Task Generator (every 3 hours)","text":"<pre><code># db96 Task Generator file path\ncd /home/s159123/gitrepo/dataop/agents/agent_oracle_puller_queue\npython3 append_to_crawl_queue.py\n</code></pre> <ul> <li>Cron Scheduling Command</li> </ul> <pre><code># db96\n0 2,5,8,11,14,17,20,23 * * * cd /home/s159123/gitrepo/dataop/agents/agent_oracle_puller_queue &amp;&amp; python3 append_to_crawl_queue.py --append_to_queue --githash '3110c63' --day_partition_number 8\n</code></pre> <ul> <li>Purpose<ul> <li>Populate/Produce the tasks queues for <code>oracle_puller</code> to pull data the DB96 every 3 hours</li> </ul> </li> <li>Explanation<ul> <li><code>0</code>: minutes</li> <li><code>2,5,8,11,14,17,20,23</code>: hours, 8 times in a day for the crontab job to  launch the following script</li> <li><code>* * *</code>: every day; day, month, year</li> <li><code>cd /home/s159123/gitrepo/dataop/agents/agent_oracle_puller_queue</code>: go to the working folder</li> <li><code>python3 append_to_crawl_queue.py --append_to_queue --githash '3110c63' --day_partition_number 8</code> : find and append the task queue  every 3 hours for <code>oracle_puller.service</code>   (running continuously) to pull data from the <code>DB96</code></li> </ul> </li> <li>More details about <code>append_to_crawl_queue.py</code><ul> <li>Purpose: Populate the crawl queue with new SQL statements. This process is similar to creating a table in a spreadsheet and populating it, but instead of storing the queue in Excel, it's stored in a Kafka service.</li> <li>Service topic:<code>SeCRAW_QUEUE_KAFKA_TOPIC = \"oracle_crawl_queue_topic\"</code><ul> <li>The same topic corresponds to the same task queue</li> <li>Kafka service distributes the tasks</li> </ul> </li> <li>Server cluster: <code>KAFKA_SERVERS=(PLAINTEXT://svgcmdl03:9092, PLAINTEXT://svgcmdl04:9092,PLAINTEXT://svgcmdl05:9092)</code></li> <li> <p>SQL template:</p> <ul> <li>Purpose: SQL command that enables the puller to extract data from the Oracle database</li> <li>Interpretation: Each SQL query represents a distinct task to be executed</li> <li>On the puller side: The system performs parameter replacement to generate the final SQL query. Read the SQL statement in the SQL queue one by one, and consume it.</li> <li>Google Drive Excel Example (task queue): https://drive.google.com/drive/u/0/home</li> </ul> </li> </ul> </li> </ul>"},{"location":"data/data-relay/#dwo-task-generator-daily","title":"DWO Task Generator (Daily)","text":"<pre><code># DWO Task Generator file path\ncd /home/s159123/gitrepo/dataop/agents/agent_oracle_puller_queue\npython3 append_to_crawl_queue_config.py\n</code></pre> <ul> <li>Cron Scheduling Command</li> </ul> <pre><code># DWO config crawl task daily\n0 19 * * * cd /nfsdata/dataop/uploader &amp;&amp; python3.9 append_to_crawl_queue_config.py --githash 'ccccccc' --append_to_queue\n</code></pre> <ul> <li>Purpose<ul> <li>Populate/Produce the task queues for <code>oracle_puller</code> to pull configuration data from the DWO database daily</li> </ul> </li> <li> <p>Explanation</p> <ul> <li>refer other similar detailed explanation to the db96</li> </ul> <p>Some other commands for testing</p> <pre><code>python3.9 append_to_crawl_queue.py --output_csv_file db96.text.csv --day_partition_number 8 --crawl_window_start \"2024-03-01 18:00:00\" --githash default\nvi db96.text.csv\npython3.9 append_to_crawl_queue.py --output_csv_file dwo.text.csv --day_partition_number 8 --crawl_window_start \"2024-03-01 18:00:00\" --githash default\nvi dwo.text.csv\n</code></pre> </li> </ul>"},{"location":"data/data-relay/#data-uploadersconsumers-on-svgcmdl02-d2","title":"Data Uploaders/Consumers on svgcmdl02 (D2)","text":"<p>Result/Output</p> <ul> <li>Data uploaded to AWS/Snowflake</li> </ul> <p>Relation with <code>oracle_puller</code> :</p> <ul> <li><code>oracle_puller</code>: Data (json) generator/ producer (svgcmdl01)</li> <li><code>crontab controller</code>: Data (json) executor/consumer (svgcmdl02)</li> </ul>"},{"location":"data/data-relay/#db96-data-uploader-for-aws-every-3-hours","title":"db96 Data: Uploader for AWS (every 3 hours)","text":"<pre><code># gateway configuration\n/nfsdata/dataop/uploader/aws_upload_daemon_script.sh\n</code></pre> <ul> <li>Cron Scheduling Command</li> </ul> <pre><code># Cron scheduling the aws uploader, every 6 hours. (consume the data pulled from the oracle_puller, upload PeMS data to AWS)\n0 0,6,12,18 * * * sh /nfsdata/dataop/uploader/aws_upload_daemon_script.sh \"D3.VDS30SEC,D4.VDS30SEC,D5.VDS30SEC,D6.VDS30SEC,D7.VDS30SEC,D8.VDS30SEC,D10.VDS30SEC,D11.VDS30SEC,D12.VDS30SEC\"\n</code></pre> <ul> <li>Purpose<ul> <li>Upload the data extracted by <code>oracle_puller</code> to AWS/Snowflake every 6 hours</li> </ul> </li> <li>Explanation<ul> <li>refer other similar detailed explanation to the db96 task generator</li> </ul> </li> </ul>"},{"location":"data/data-relay/#dwo-config-data-uploader-for-aws-every-3-hours","title":"DWO Config Data Uploader for AWS (every 3 hours)","text":"<pre><code>/nfsdata/dataop/uploader/aws_upload_daemon_script.sh\n</code></pre> <ul> <li>Cron Scheduling Command</li> </ul> <pre><code># DWO config data upload daily (upload config data to AWS)\n0 23 * * * sh /nfsdata/dataop/uploader/aws_upload_daemon_script.sh \"CONTROLLER_CONFIG,CONTROLLER_CONFIG_LOG,DETECTOR_CONFIG,DETECTOR_CONFIG_LOG,STATION_CONFIG,STATION_CONFIG_LOG\"\n</code></pre> <ul> <li>Purpose<ul> <li>Upload the config data extracted by <code>oracle_puller</code> to AWS/Snowflake daily</li> </ul> </li> <li>Explanation<ul> <li>refer other similar detailed explanation to the db96  task generator</li> </ul> </li> </ul>"},{"location":"data/data-relay/#kafka-service","title":"Kafka Service","text":""},{"location":"data/data-relay/#purpose","title":"Purpose","text":"<ul> <li>Centralized service to manage the supplier and consumer</li> <li>Hosting the crawl queue</li> </ul> <pre><code>cd /home/s159123/gitrepo/dataop/agents/agent_oracle_puller_queue\n</code></pre> <ul> <li> <p>Perform a health check for the Kafka service by verifying if each server is running</p> <pre><code>telnet svgcmd102/3/4/5 9092\n</code></pre> </li> </ul>"},{"location":"data/data-relay/#different-kafka-topics","title":"Different Kafka Topics","text":""},{"location":"data/data-relay/#1-kafka-topic-for-crawl-tasks","title":"1. kafka topic for crawl tasks","text":"<p><code>oracle_crawl_queue_topic</code> (coordination between the crawl task generator and puller)</p>"},{"location":"data/data-relay/#2-kafka-topics-for-data-buffer","title":"2. kafka topics for data buffer","text":"<ul> <li>Kafka topic: <code>D3.VDS30SEC</code></li> <li>Kafka topic: <code>D4.VDS30SEC</code></li> <li>\u2026</li> <li>Kafka topic: <code>D12.VDS30SEC</code></li> </ul> <p>Puller (every 10 minutes):</p> <p>Uploader (every 6 hours)</p> <ol> <li><code>df = pd.read_sql(\"select * from D3.VDS30SEC \u2026\")</code> (Notes: * is too heavy, 10-minute data every time, small batch pulling continuously)</li> <li>puller then transfer the <code>df</code> then upload to kafka topic: <code>D3.VDS30SEC</code> (in the format of json)</li> </ol> <p>Uploader on D2 svgcml02 will be able to consume the data in <code>D3.VDS30SEC</code> and batch uploading (with transform) it to snowflake\u2019s <code>D3.VDS30SEC</code></p>"},{"location":"data/data-relay/#source-of-data-relay-task-details","title":"Source of Data Relay [Task Details]","text":""},{"location":"data/data-relay/#pems-oracle-database-db96-dwo","title":"PeMS Oracle Database (DB96, DWO)","text":"<ul> <li>DB96 (Headquarter database for storing the field sensor data from district TMCs)<ul> <li>Data resource<ul> <li><code>D3.VDS30SEC</code></li> <li><code>D4.VDS30SEC</code></li> <li>\u2026</li> <li><code>D12.VDS30SEC</code></li> </ul> </li> <li>Columns for D3.VDS30SEC:<ul> <li><code>SAMPLE_TIME</code></li> <li><code>LOOP1_SPD</code></li> <li><code>LOOP1_VOL</code></li> <li><code>LOOP2_SPD</code></li> <li><code>LOOP2_VOL</code></li> <li>\u2026</li> <li><code>LOOP8_SPD</code></li> <li><code>LOOP8_VOL</code></li> </ul> </li> <li>Credentials in <code>/home/jupyter/.bashrc on svgcmdl01</code> (i.e. <code>DB_USER</code> <code>DB96_PASSWORD</code>)</li> <li>Test some simple query <code>query_db96_debug.py</code>:<ul> <li><code>cd /data/projects/crawler/agent_oracle_puller_queue/</code></li> <li><code>python3.9 query_db96_debug.py</code><ul> <li><code>Get_Data_From_PeMS(dbname: DB96/DWO,  sql statement)</code> :</li> <li>To stop the process: Use <code>ps -ef | grep query</code> to find the process ID, then <code>kill [process_id]</code> (e.g., <code>kill 66472</code>)</li> </ul> </li> </ul> </li> </ul> </li> <li>DWO<ul> <li>Six Tables<ul> <li><code>PEMS.STATION_CONFIG</code></li> <li><code>PEMS.STATION_CONFIG_LOG</code></li> <li><code>PEMS.CONTROLLER_CONFIG</code></li> <li><code>PEMS.CONTROLLER_CONFIG_LOG</code></li> <li><code>PEMS.DETECTOR_CONFIG</code></li> <li><code>PEMS.DETECTOR_CONFIG_LOG</code></li> </ul> </li> <li>Credentials in <code>/home/jupyter/.bashrc on svgcmdl01</code> (i.e. <code>DB_USER</code> <code>DB96_PASSWORD</code>)</li> </ul> </li> <li>Other Path:<ul> <li>SQL Developer (Ken)</li> </ul> </li> </ul>"},{"location":"data/data-relay/#target-of-data-relay-task-details","title":"Target of Data Relay [Task Details]","text":"<p>Purpose: Check the output/target of the data relay</p>"},{"location":"data/data-relay/#how-to-navigate","title":"How to navigate","text":"<ul> <li>Login to <code>ssh s159123@svgcmdl02.dot.ca.gov</code></li> <li><code>cd /nfsdata/dataup/uploader/</code></li> <li>Write some SQL statement such as the ones in <code>snowsql/scratch.sql</code> (exemplary SQL statements)</li> <li> <p>Procedure to run:</p> <ul> <li> <p>Step 1: Load snowflake credentials <code>SNOWSQL_PWD</code> in <code>/home/s159123/.bashrc</code> to the environment:</p> <pre><code>source /home/s159123/.bashrc\n</code></pre> </li> <li> <p>Step 2.0: Use Snowsql command (test):</p> <pre><code>REQUESTS_CA_BUNDLE=/etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt\nsnowsql -a NGB13288 -u MWAA_SVC_USER_DEV -o insecure_mode=True -f\n/nfsdata/dataop/uploader/snowsql/scratch.sql -d RAW_DEV\n</code></pre> </li> <li> <p>Step 2.1: Use Snowsql command (real program):</p> <pre><code>REQUESTS_CA_BUNDLE=/etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt\nsnowsql -a NGB13288 -u MWAA_SVC_USER_DEV -o insecure_mode=True -f\n/nfsdata/dataop/uploader/snowsql/snowsql_load_CONTROLLER_CONFIG_LOG.sql -d RAW_DEV\n</code></pre> </li> </ul> </li> <li> <p>Snowflake DBs (Transformed data)</p> <ul> <li>Transformed DB96 tables:<ul> <li>DB96.VDS30SEC</li> </ul> </li> <li>Transformed DWO tables:<ul> <li>Six Tables<ul> <li><code>DB96.CONTROLLER_CONFIG</code></li> <li><code>DB96.CONTROLLER_CONFIG_LOG</code></li> <li><code>DB96.STATION_CONFIG</code></li> <li><code>DB96.STATION_CONFIG_LOG</code></li> <li><code>DB96.DETECTOR_CONFIG</code></li> <li><code>DB96.DETECTOR_CONFIG_LOG</code></li> </ul> </li> </ul> </li> <li>website: <code>https://app.snowflake.com/vsb79059/dse_caltrans_pems/worksheets</code></li> </ul> </li> </ul>"},{"location":"data/data-relay/#intermediate-data-task-details","title":"Intermediate data [Task Details]","text":"<ul> <li>Read the local parquet file: <code>/nfsdata/dataop/uploader/tmp/CONTROLLER_CONFIG_LOG_dump_static.parquet @~/controller_config_log;</code><ul> <li>Read this parquet file <code>pd.read_parquet(/nfsdata/dataop/uploader/tmp/CONTROLLER_CONFIG_LOG_dump_static.parquet, dtype_backend='pyarrow')</code></li> </ul> </li> <li>Transform the parquet file to snowflake tables</li> </ul>"},{"location":"data/data-relay/#faq","title":"FAQ","text":""},{"location":"data/data-relay/#foundational-concepts","title":"Foundational concepts","text":"<ol> <li>Cron Jobs:</li> <li>Definition: Cron jobs are scheduled tasks that run automatically at specified intervals. In this workflow, they are responsible for generating task queues that trigger data pulls from the Oracle database.</li> <li> <p>Example: A cron job might be set to run every 3 hours to initiate a data pull process via Kafka.</p> </li> <li> <p>Kafka:</p> </li> <li>Definition: Kafka is a distributed event streaming platform used for building real-time data pipelines and streaming applications. It allows different services to communicate asynchronously by sending messages through topics.</li> <li> <p>In this context: Kafka serves as a central service for managing tasks and buffering data between producers (task generators) and consumers (data processors).</p> </li> <li> <p>Oracle Puller:</p> </li> <li>Definition: A custom script or daemon (e.g., <code>oracle_puller_daemon.py</code>) that extracts data from an Oracle database based on predefined tasks (e.g., SQL queries). The data is then sent to Kafka for further processing.</li> <li> <p>Function: It continuously pulls data from Oracle as tasks are generated, ensuring up-to-date data is retrieved.</p> </li> <li> <p>Task Queue:</p> </li> <li>Definition: A list or queue of tasks that need to be processed by a specific service. In this case, the task queue is generated by cron jobs and sent to Kafka to be consumed by the <code>oracle_puller</code>.</li> <li> <p>In this context: The tasks in the queue are SQL queries that the <code>oracle_puller</code> executes to retrieve data from Oracle.</p> </li> <li> <p>Data Buffer:</p> </li> <li>Definition: A temporary storage area where data is held before being processed or transferred to its final destination. In this workflow, Kafka acts as a buffer for the data pulled from Oracle, ensuring smooth data flow to the uploader.</li> <li> <p>Purpose: Buffers prevent data loss or overload by holding data temporarily, especially during peak loads or system delays.</p> </li> <li> <p>AWS S3:</p> </li> <li>Definition: Amazon Simple Storage Service (S3) is an object storage service used for storing and retrieving any amount of data at any time. It's scalable, reliable, and widely used for cloud data storage.</li> <li> <p>In this workflow: Data pulled from Oracle is uploaded to AWS S3 as a final storage destination after being processed.</p> </li> <li> <p>Snowflake:</p> </li> <li>Definition: Snowflake is a cloud-based data warehousing platform that allows organizations to store, process, and analyze large volumes of structured and semi-structured data.</li> <li> <p>In this workflow: Snowflake is used as another storage destination for the pulled data, enabling further analysis and reporting.</p> </li> <li> <p>SQL Queries:</p> </li> <li>Definition: Structured Query Language (SQL) is used to communicate with and manipulate databases. SQL queries retrieve, update, or manage data stored in relational databases.</li> <li> <p>In this context: SQL queries are generated by cron jobs and sent to the <code>oracle_puller</code>, which uses them to extract data from the Oracle database.</p> </li> <li> <p>Daemon:</p> </li> <li>Definition: A daemon is a background process that runs continuously and performs specific operations without user interaction.</li> <li> <p>In this context: The <code>oracle_puller_daemon.py</code> script is a daemon that constantly monitors for tasks from Kafka, pulls data from Oracle, and forwards it to Kafka topics.</p> </li> <li> <p>Buffer Topics (Kafka Topics):</p> <ul> <li>Definition: In Kafka, topics are named channels where data records (messages) are published and from which consumers read. Topics are partitioned and distributed for scalability.</li> <li>In this workflow: Buffer topics (e.g., <code>D3.VDS30SEC</code>) temporarily hold the data pulled from Oracle before it is uploaded to AWS or Snowflake.</li> </ul> </li> <li> <p>Producers and Consumers:</p> <ul> <li>Producer: In Kafka, a producer is a service or application that sends data to a Kafka topic.</li> <li>Consumer: A consumer reads data from a Kafka topic and processes it further.</li> <li>In this workflow: svgcmdl02 acts as a producer of task queues, while svgcmdl01 is a consumer of tasks (SQL queries). Similarly, svgcmdl01 produces data, and svgcmdl02 consumes it for uploading.</li> </ul> </li> </ol>"},{"location":"data/data-relay/#task-related-questions","title":"Task Related Questions","text":"<ol> <li>What is the purpose of the <code>oracle_puller_daemon.py</code> script?</li> <li> <p>The <code>oracle_puller_daemon.py</code> script is used to pull data from the Oracle database. It operates continuously to retrieve and process data based on tasks assigned via Kafka.</p> </li> <li> <p>How often do cron jobs generate tasks for the <code>oracle_puller</code>?</p> </li> <li> <p>Cron jobs generate tasks every 3 hours for the <code>oracle_puller</code> to execute. This scheduling ensures that data is pulled from the Oracle database at regular intervals.</p> </li> <li> <p>What role does Kafka play in the data workflow?</p> </li> <li> <p>Kafka serves as a central messaging service that manages the flow of tasks and data between different components. It stores task queues and buffers data before it is consumed by the uploader.</p> </li> <li> <p>How is data transferred from Kafka to AWS S3/Snowflake?</p> </li> <li> <p>Data is transferred from Kafka to AWS S3/Snowflake by the <code>aws_upload_daemon_script.sh</code>, which runs every 6 hours. This script consumes the data buffered in Kafka topics and uploads it to the final storage destinations.</p> </li> <li> <p>What is the significance of the data buffer topics in Kafka?</p> </li> <li> <p>Data buffer topics in Kafka (e.g., <code>D3.VDS30SEC</code>, <code>D4.VDS30SEC</code>) are used to temporarily store the pulled data before it is processed and uploaded. These topics help manage data flow and ensure that data is handled efficiently.</p> </li> <li> <p>How do cron jobs interact with Kafka?</p> </li> <li> <p>Cron jobs on svgcmdl02 generate SQL tasks and publish them to Kafka topics. These tasks are then consumed by the <code>oracle_puller</code> on svgcmdl01, which pulls the data from the Oracle database.</p> </li> <li> <p>What happens if there is a failure in the data pull process?</p> </li> <li> <p>In case of a failure, the <code>oracle_puller</code> will not be able to retrieve data, which could affect the subsequent steps. Error handling and retry mechanisms should be in place to address failures and ensure data integrity.</p> </li> <li> <p>How is the scheduling of data uploads managed?</p> </li> <li> <p>Data uploads are managed by the <code>aws_upload_daemon_script.sh</code>, which is scheduled to run every 6 hours. This script handles the transfer of data from Kafka to AWS S3/Snowflake.</p> </li> <li> <p>Can the frequency of cron jobs or data uploads be adjusted?</p> </li> <li> <p>Yes, the frequency of cron jobs and data uploads can be adjusted by modifying the respective cron job schedules and script settings as needed.</p> </li> <li> <p>What are the key components involved in this data workflow?</p> <ul> <li>The key components are the Oracle database, <code>oracle_puller</code> script, Kafka service, cron jobs, and AWS S3/Snowflake. Each plays a crucial role in managing and processing the data.</li> </ul> </li> </ol>"},{"location":"data/data-transform/","title":"Data Transformation Process","text":"<p>This page details the data transformation processes for the PeMS Modernization Project. Data from the sources identified below are transformed from their raw format into calculated performance metrics that are loaded into tables and files that can used for reporting, visualizations, and/or be downloaded by users.</p> <p>There are three primary data sources that the PeMS Modernization Project uses for analysis, reporting and visualizations:</p> <ol> <li>PeMS receives field collected detector raw 30-second data from Intelligent    Transportation System (ITS) Vehicle Detector Stations (VDS) relayed by the District TMC's</li> <li>Caltrans Districts provide station configuration information primarily through manual    uploads of XML files through the PeMS website.    (i.e. Location, identification ID's, and other metadata for stations/detectors/controllers)</li> <li>Freeway configuration information for visualizations is obtained from the Caltrans Linear Referencing    System is obtained from the Caltrans GIS Data Open Portal website    (i.e. district, county, route, postmiles, etc.)</li> </ol> <p>Caltrans HQ staff are currently in the process of building data pipelines to bring in other datasources including CHP incident and Lane Closure System data.</p>"},{"location":"data/data-transform/#transformation-processes","title":"Transformation Processes","text":""},{"location":"data/data-transform/#vehicle-detector-stations-vds-configuration-transformation","title":"Vehicle Detector Stations (VDS) Configuration Transformation:","text":"<p>District staff uploads station configuration data via XML files which are processed and transformed into various tables located within the Oracle data warehouse schemas. A data pieline takes the station configuration data from Oracle and loads them into our AWS/Snowflake data warehouse for further processing into staging data models. The station configuration data is later combined with the raw 30-second data in the following section.</p>"},{"location":"data/data-transform/#30-second-raw-data-transformation","title":"30-Second Raw Data Transformation","text":"<ol> <li>The 30-second raw data is received from district TMC's and transformed into tables that are divided by district. The raw    data is received in a number of different formats including via SQLnet, XML feed over TCP, and in others raw controller    packets via RPC.</li> <li>A data pipeline takes the data from the Oracle data lake tables and places it in the Snowflake RAW database. Additional    details and a flow diagram are included here: data loading documentation</li> <li>The raw data is then transformed in staging data models and remains aggregated at the station level. The VDS data is also    transformed in staging data models.</li> <li>The 30-second raw data is then aggregated to 5 minute samples per lane in our intermediate data model. For detectors that    do not report sample data or where data is missing during a 5-minute interval. The goal of these data models is to capture    5-minute intervals for all stations/detectors that are considered active on any given date for all 24 hours.</li> <li> <p>The 5-minute data aggregations are then processed in the following order:</p> <ul> <li>If 10 or more samples are reported in a 5-minute timeframe the sum of the flow value is checked to ensure it is between 0   and the 5-minute max capacity value. If the flow value meet this condition it is used. The average occupancy values are checked   to ensure they are between 0 and 1. If the occupancy value meets this condition it is used. If a speed value is reported by the   device the flow weighted speed value is calculated and used.</li> <li>If between 1-9 samples are reported in a 5-minute timeframe the flow data is normalized as if 10 samples were received using the   following formula: (reported flow ) * (10 / number of samples reported). The resulting value for the normalized flow is used. The   average of the reported occupancy values is used, no normalization is done for occupancy or speed. The flow weighted speed value is   used based on the raw data provided, no normalization is done with speed data.</li> <li>If the flow or occupancy values fail any of the checks above or if the detector is diagnosed as BAD based on the diagnostic tests,   than the flow, occupancy and speed are imputed. The data models related to imputation are intermediate models contained in the   IMPUTATION schema.</li> </ul> </li> <li> <p>Once the data 5-minute data aggregations for flow, occupancy and speed are completely populated, performance metrics are calculated    at the detector/lane level. The data is then aggregated to the station level with the following exception: the delay for a station is    based on the aggregated speed and flow across all lanes. It is NOT the sum of the individual lane delay for a station.</p> </li> <li>For individual lanes and stations, the complete 5-minute data (observed and imputed) is aggregated across multiple temporal    (hourly, daily, weekly, monthly) aggregations. The intermediate data models that contain the various temporal metrics are contained    in the PERFORMANCE schema.</li> <li>Detector diagnostic tests are performed on daily data based on the sample data received or not received. Additional details of    the diagnostic tests performed can be found here: detector health diagnostics</li> <li>Once the data processing is completed in the intermediate data models, mart data models are built from the intermediate models so    they can used for reporting on the web or in our visualization tools.The data models in the marts folder are calculated daily and are    located in the ANALYTICS_PRD database and a copy is also saved to a public S3 bucket.</li> <li>Please note that additioanl details for imputation, bottlenecks and other calculations are included in other sections of this website.</li> </ol>"},{"location":"data/detector-health/","title":"Detector Health Diagnostics","text":"<p>Vehicle detectors are the main source of data in the system and provide the best source of real-time traffic data available to Caltrans. Data from the detectors is relayed from the field to District TMC's which ultimately makes its way to the system. The detector data is combined with equipment data (i.e. detector id, district, county, route, postmile, description, etc.) in order to calculate performance measures and aggregate them spatially and temporally. Since the detector data serves as the source for most of the performance calculations, the resulting performance measures are only as accurate as the underlying data.</p> <p>The Detector Health Diagnostic checks performed in the system serve multiple purposes:</p> <ol> <li>Ensure detectors and other equipment are functioning properly</li> <li>Serves as a data quality check against data being reported by detectors</li> <li>Assists Caltrans staff in troubleshooting and maintaining detectors/stations/controllers</li> <li>Used as a data quality measure for imputation, bottleneck and other calculations</li> </ol> <p>Caltrans has installed detectors to cover most urban freeways on the State Highway System. The data collected, however, can contain missing values (also referred to as \"holes\") or \"bad\" (incorrect) values that require data quality analysis to ensure the highest level of data quality is being used to produce reliable results. The system contains various data quality checks to detect when data is \"bad\" to ensure the \"bad\" data is not used to compute performance measures. When \"bad\" or missing data is encountered, the system imputes values to fill these data \"holes\". The resulting database of complete and reliable data ensures that analyses produce meaningful results. The system relies on extensive analysis of all of the detectors in the state to help approximate what the true values would be had the faulty detectors been working properly and sending good data. The ability of the system to adjust for missing data is a real advantage in using the system for performance measurements.</p> <p>For every analysis performed in the system, confirming the data quality is essential. The basic idea of the diagnostic tests are to identify \"bad\" detectors based on the observed measurements of volume and occupancy over an entire day. We do this by computing summary statistics from the flow and occupancy measurements every day. The following tests have been tested, adjusted, and re-adjusted to yield effective results. The algorithms based on the tests have their historical roots in a paper by Chen, Kwon, Skabardonis and Varaiya (members of the the system original development team). This paper, \"Detecting errors and imputing missing data for single loop surveillance systems,\" is available on the Internet. The algorithms that the system uses have been modified from the original version described in the paper as a result of Caltrans\u2019 experience operating the system for more than a decade.</p> Test Number Detector Types Condition Description Diagnostic Test Data Used Diagnostic 1 All including Ramps Never receive any data samples The system breaks down this condition into five bins based on the communication infrastructure. The first bin indicates that none of the detectors in the same district as the selected detector are reporting data. When a district feed is down all of the data calculated in the system for that day in the district is imputed. Number of samples received is equal to zero for all detectors in the district. 30-sec District Feed Down The second bin indicates that none of the detectors attached to the same communication line as the selected detector are reporting data. Note that information about communication lines is not always available. In this case, this test is omitted. Number of samples received is equal to zero for all detectors attached to the same communication line. 30-sec Line Down The third bin indicates that none of the detectors attached to the same controller as the selected detector are reporting data. This probably indicates no power at this location or the communication link is broken. Number of samples received is equal to zero for all detectors attached to the controller. If communication line information is available, then at least one other controller on the same line is reporting data. 30-sec Controller Down The fourth bin indicates that none of the detectors attached to the same station as the selected detector are reporting data. Number of samples received is equal to zero for all detectors attached to the same station. 30-sec Station Down The fifth bin indicates that the individual detector is not reporting any data, but other detectors on the same controller are sending samples. This most likely indicates a software configuration error or bad wiring. Number of samples received is equal to zero, but other detectors on the same controller are reporting data. 30-sec No Data 2 All including Ramps Too few data samples The system received some samples but not enough to perform diagnostic tests. Other detectors reported more samples (so the data feed did not die). # of samples &lt; 60% of the max collected samples during the test period. 30-sec Insufficient Data 3 All including Ramps Zero occupancy or flow There are too many samples with an occupancy (non-ramps only) or flow (Ramps only) of zero. The system suspects that the detector card (in the case of standard loop detectors) is off. Non-Ramps: # zero occ samples &gt; % of the max collected samples during the test period.Ramp: # zero flow samples &gt; % of the max collected samples during the testperiod. 30-sec Card Off 4 All including Ramps High values There are too many samples with either occupancy above 0% (non-ramps only) or flow above veh/30-sec (Ramps only). The detector is probably stuck on. Non-Ramps: # high occ samples &gt; % of the max collected samples during the test period.Ramp: # high flow samples &gt; % of the max collected samples during the testperiod. 30-sec High Value 5 All excluding Ramps Flow-Occupancy mismatch There are too many samples where the flow is zero and the occupancy is non- zero. This could be caused by the detector hanging on. # of flow-occupancy mismatch samples &gt; % of the max collected samples during the test period. 30-sec Intermittent 6 All excluding Ramps Occupancy is constant The detector is stuck at some value for some reason. The system knows that occupancy should have some variation over the day. the system count the number of times that the occupancy value is non-zero and repeated from the last sample (is exactly the same as the last sample). # repeated occupancy values &gt; 5-min samples. 5-min Constant Occupancy <p>The steps that are involved in actually implementing the complete diagnostic algorithm are as follows:</p> <p>We compute the statistics needed for the above tests over the time period from 5am until 10pm (we don't want to capture the time period when there are very little vehicles on the freeway anyways) every day. For each of the above tests, we check the statistics against the predefined thresholds. The threshold values are located in the systems Seed files. If any of the tests for a detector fails, then the detector is declared \"bad\" and we stop testing. We record that the detector is declared as \"bad\" in the database. Users can subsequently view tables of \"bad\" detectors. Once a mainline or HOV detector is identified as \"bad\", we impute data in order to fill in for the \"bad\" detector for the same day the detector is diagnosed as \"bad\". Note that we do not impute for any ramp detectors. It is important to note that we do not identify individual data samples as bad or good. We make this determination on a detector-by-detector basis each day.</p> <p>We perform the diagnostic tests on all detectors in the system including on and off ramps. The types of tests that we apply are different for each type of detector. For example, most mainline detectors report flow and occupancy whereas ramp detectors usually only report flow. Hence for ramps we can only do a subset of the tests that we perform for mainline detectors.</p> <p>It is important to note that we are attempting to identify \"bad\" detectors, not bad data samples. The system performs a number of simple filters on individual data samples just to make sure that they make sense as a measurement (e.g., no values less than zero, certain values can not be null, flow values do not exceed a maximum values determined by statistical analysis, etc.) The system does not perform real-time checks.</p> <p>Detectors can go \"bad\" or malfunction for many reasons. This is typically an intermittent or recurrent problem that can require different approaches to properly diagnose and fix. The system devotes a substantial amount of its computing resources to identifying \"bad\" detectors and calculating health diagnostics to help users evaluate data quality and to help those responsible for detector maintenance.</p> <p>There are many potential points of failure in this process, which can include the physical devices themselves (i.e. hardware, equipment, device, communications links, etc.) or can also involve non-physical, \u201chuman error,\u201d such as making errors in the configuration of the device such as associating the wrong county, route and postmile with a station, or assigning the wrong identification number to a station. These human errors can be difficult to diagnose.</p>"},{"location":"data/detector-health/#understanding-errors-when-no-data-is-received","title":"Understanding Errors When No Data is Received","text":"<p>The systems detector health diagnostic approach is to assign a failure status to individual detectors. When the system receives data samples, it can test the samples against the threshold values to determine a number of different types of errors. But when the system receives no data samples it's difficult to tell if the detector itself is \"bad\". If we know the physical data collection infrastructure, which means knowing which detectors are connected to which stations, or which stations are connected to which controllers we can focus our diagnostics to the equipment level.</p> <p>For example, it could be that we aren't receiving any data samples from any detector simply because the FEP is down. For situations like this we mark the detector as \"bad\" with a reason of district feed down. This status simply represents that we didn't receive any data samples in the district for a given date. The reason we don't mark the detector as \"good\" is due to how \"good\" detector data is used in other system calculations. For example, we only perform certain performance metrics such as bottleneck and congestion analyses where we have \"good\" detectors. We do not believe it is appropriate to include imputed detector data in these advanced metrics, especially since the imputation methods used when a district feed is down are the least robust methods due to a lack of observed data.</p> <p>As another example, if we know that all of the detectors that are connected to a single station are not sending any data samples, then it's likely that the issue is related to the station. It could also mean that all of the detectors connected to the station have issues but that is less likely. If some detectors are reporting samples to a station and other detectors are not reporting samples then the detectors not reporting samples are the likely source of the issue. If a controller is not receiving any samples from connected stations there could be no power at the controller, or it could have been damaged in an accident (or removed during construction).</p> <p>If we receive samples from some stations connected to a controller but not other stations, then the station is considered to be down and the loops belonging to those controllers are marked as station down. If we receive samples from some detectors but not others for a single station then the non-reporting detectors are considered to be down but not the station and are marked as no data. It should be pointed out that we're assuming that the district feed, controller or stations are the problem if we don't receive data samples from the detectors underneath them. But it could be that all of the individual loops are broken although this is considered unlikely.</p> <p>In districts where we don't have the physical topology of the data collection infrastructure, or the data collection infrastructure is just a star the detector status checks are more limited. District 4 doesn't provide us with their data collection infrastructure and District 10 has only wireless modems that are sending data back to a centralized point so not all of the detector status checks are possible.</p>"},{"location":"data/exploratory-data-analysis/","title":"Caltrans PeMS: Exploratory Data Analysis","text":""},{"location":"data/exploratory-data-analysis/#background","title":"Background","text":"<p>The California Department of Transportation (Caltrans) collects data that describes the flow of traffic on California freeways. Caltrans stores these data in a database called PeMS. The data describe the number of counts per unit time measured by roughly 45,000 sensors on a 30-second cadence. The type of sensor varies considerably, e.g. radar and magnetometers (see Chapter 1 of the Introduction to PeMS User Guide).</p> <p>In some cases, these data are missing. Faulty or broken sensors do not collect data. Or sensor data is not wirelessly transmitted back to PeMS. In addition, Caltrans performs some calculations to convert these raw sensor data into physical observables such as speed. These calculations include some assumptions such as the length of the vehicle, or g. Based on the quality of the assumption, these data can include errors.</p>"},{"location":"data/exploratory-data-analysis/#goals","title":"Goals","text":"<p>The goals for the project were two-fold: to learn how to use Snowflake to analyze large datasets and to answer a few basic questions about the nature of the data. We developed an interactive data visualization to quickly explore the data.</p>"},{"location":"data/exploratory-data-analysis/#findings","title":"Findings","text":"<p>We report three main findings about the nature of the data.</p> <ol> <li>Do some types of roads have more missing data than others? The Caltrans/PeMS sensor data describes many different types of roads. Some are interstate roads, while others are small, local roads. The California Department of Transportation identifies seven major types of roads. The data we have does not specify the type of road. There is a \u2018FWY\u2019 column for each station that contains an integer road number. The 15, 10, and 5 freeways have the most sensors.</li> <li>Do some regions have more missing data than others? The Caltrans/PeMS data is organized by districts. This map shows how Caltrans carves the state into 11 districts. An analysis of data in 2022 and 2023 shows 19.2 thousand sensors with almost 28 billion observations. Some 5.6% of the sensors had all null observations (<code>FLOW_1</code>) and 9.1% of the sensors had all zero observations (<code>FLOW_1</code>) . We observed a few different types of missing data: single missing observations, intermittent outages for a few minutes during the day, and hours to months long outages.</li> <li>Do the calculations for speed and traffic flow make sense? The Caltrans/PeMS data uses some heuristics to calculate speed and traffic flow. Are these reasonable? To answer this question, we can try to confirm some basic traffic patterns. For example: Does traffic increase during peak traffic hours, which is 6am to 10am and again from 3pm to 7pm in Northern California? While there are some anomalous observations, the flow and speed generally follow a pattern with low speeds and high flow during peak traffic hours. See Figure 6 for an example.</li> </ol> <p></p> <p>Figure 1. An example of a null observation.</p> <p></p> <p>Figure 2. An example of an observation containing all zero values.</p> <p></p> <p>Figure 3. Average lane 1 flow from all California sensors in 2022 and 2023. Black dots represent sensors with all null values.</p> <p></p> <p>Figure 4. Sensors reporting all null values for 2022 and 2023.</p> <p></p> <p>Figure 5. Sensors reporting all zero values for 2022 and 2023. These appear to be centered around the major metropolitan areas.</p> <p></p> <p>Figure 6. The speed (left) and flow (right) of traffic over the Bay Bridge on Tuesday, September 13, 2022.</p>"},{"location":"data/exploratory-data-analysis/#deliverables","title":"Deliverables","text":"<ul> <li> <p>Deliverable 1: We learned how to use Snowflake to analyze large datasets. To read data from Snowflake, we used an open-source Python package called Ibis. Ibis provides a Python interface to Snowflake that's easy to use. Ibis loads data from a Snowflake into a pandas dataframe. pandas dataframes connect nicely to a typical data science workflow. We demonstrated this workflow in this notebook, located in the <code>/pems</code> subdirectory.</p> </li> <li> <p>Deliverable 2: This documentation.</p> </li> <li> <p>Deliverable 3: An interactive data visualization. We ultimately chose to go with building the dashboard via Dash. This was mainly due to the platform's maturity, robust documentation, and known flexibility (see Table 1). While Dash provided a working solution that was performant enough to work with, the poor deployment options are a disappointment. To make deployment of these Dash apps workable, it would require the team to adopt this as a standard. This is because it requires standing up a server, or paying plotly to host it. Streamlit appears to check most of the boxes; however, its scalability and performance is the only open question. Shiny for Python appears to be very promising; however, it is still relatively new.</p> </li> </ul> Tool Pros Cons Deployment options Dash Plotly <ul><li> Well established with good documentation and community</li><li>Simple interactive plots via plotly<li>Flexible</li> <ul><li>No simple hosting option</li></ul> <ul><li>Self hosted options</li><li>Paid options</li><li>Deploy Your Dash App | Dash for Python Documentation | Plotly</li></ul> Streamlit <ul><li>Probably simplest to get started and easiest to deploy</li><li>Integrates with Snowflake</li><li>Free and simple deployment options</li><li>Robust community <ul><li>Limited functionality and scalability compared to others</li><li>Limited in ability to take input from user interaction (e.g. clicks) with maps/visualizations</li></ul> <ul><li> Streamlit in Snowflake</li><li>Easy and free via Github</li><ul><li>No paid options for more compute and memory</li> Shiny for Python <ul><li>The R implementation is one of the most well established and mature dashboarding ecosystems for any language</li><li>Free and simple deployment options <ul><li>Relatively new for Python so documentation and community is currently limited <ul><li>Self hosted options via shiny server</li><li>Deployment Options</li><li>Easy and free via shinyapps.io</li><ul><li>Paid options for more compute and memory <p>Table 1. Comparison of different dashboard options</p>"},{"location":"data/exploratory-data-analysis/#remaining-questions","title":"Remaining Questions","text":"<p>Here are some open questions we recommend probing.</p> <ul> <li>Why would sensors not report for long periods of time? Why do some sensors have zero values for a long period of time while others report null values?</li> <li>What are the different reasons why a sensor\u2019s data may be missing? Does this differ by sensor type, location, lane, etc.? Which type of missing data requires imputation?</li> <li>Do some years or seasons have more missing data than others?</li> <li>How many total sensors are there? (There is a discrepancy between the website reporting 40K sensors and the data containing 25K sensors).</li> <li>Does the data we have include imputed data? If so, is there a way to tell what is imputed or not?</li> <li>What is reasonable in terms of a sensor missing data?</li> <li>What source of data are we using?</li> </ul>"},{"location":"data/imputation/","title":"Imputation","text":""},{"location":"data/imputation/#visualization","title":"Visualization","text":"<p>It can be difficult to evaluate different diagnostics and imputation methods without being able to easily visualize them.</p> <p>The following Python snippet uses Altair to create a chart showing the observed values of volume, occupancy, and speed for a given detector, as well as any imputed values (i.e., local regression, regional regression, and global regression).</p> <pre><code>import os\n\nimport altair\nimport snowflake.connector\n\naltair.data_transformers.disable_max_rows()\n\n# Connect to Snowflake using external browser authentication\nconn = snowflake.connector.connect(\n    user=os.environ[\"SNOWFLAKE_USER\"],\n    password=os.environ[\"SNOWFLAKE_PASSWORD\"],\n    authenticator=\"externalbrowser\",\n    account=\"NGB13288\",\n)\n\n# Sample query for a single detector over a given date range. Customize this\n# to get different detectors on different dates!\nr = conn.cursor().execute(\n    \"\"\"\n    select * from transform_prd.imputation.int_imputation__detector_agg_five_minutes\n    where station_id = '1108639' and lane = 3 and sample_date &gt;= '2024-08-04' and sample_date &lt; '2024-08-08'\n    \"\"\"\n)\ndf = r.fetch_pandas_all()\n\ndef make_chart(df):\n    \"\"\"\n    Utility Python function creating the Altair chart.\n    \"\"\"\n\n    df = df.rename(columns={\n        \"STATION_ID\": \"id\",\n        \"LANE\": \"lane\",\n        \"SAMPLE_TIMESTAMP\": \"time\",\n    })\n    selection = altair.selection_point(fields=['measurement'], bind='legend')\n\n    # Create the volume subplot\n    df_vol = df.rename(columns={\n        \"VOLUME_SUM\": \"observed\",\n        \"VOLUME_LOCAL_REGRESSION\": \"local\",\n        \"VOLUME_REGIONAL_REGRESSION\": \"regional\",\n        \"VOLUME_GLOBAL_REGRESSION\": \"global\",\n    }).melt(\n        id_vars=[\"id\", \"lane\", \"time\"],\n        value_vars=[\"observed\", \"regional\", \"local\", \"global\"],\n        var_name=\"measurement\",\n        value_name=\"volume\",\n    )\n    c_vol = altair.Chart(df_vol, width=800, height=200).mark_line().encode(\n        x=\"time:T\",\n        y=\"volume:Q\",\n        color=\"measurement:N\",\n        opacity=altair.condition(selection, altair.value(1), altair.value(0.2))\n    ).add_params(selection)\n\n    # Create the occupancy subplot\n    df_occ = df.rename(columns={\n        \"OCCUPANCY_AVG\": \"observed\",\n        \"OCCUPANCY_LOCAL_REGRESSION\": \"local\",\n        \"OCCUPANCY_REGIONAL_REGRESSION\": \"regional\",\n        \"OCCUPANCY_GLOBAL_REGRESSION\": \"global\",\n    }).melt(\n        id_vars=[\"id\", \"lane\", \"time\"],\n        value_vars=[\"observed\", \"regional\", \"local\", \"global\"],\n        var_name=\"measurement\",\n        value_name=\"occupancy\",\n    )\n\n    c_occ = altair.Chart(df_occ, width=800, height=200).mark_line().encode(\n        x=\"time:T\",\n        y=\"occupancy:Q\",\n        color=\"measurement:N\",\n        opacity=altair.condition(selection, altair.value(1), altair.value(0.2))\n    ).add_params(selection)\n\n    # Create the speed subplot\n    df_speed = df.rename(columns={\n        \"SPEED_FIVE_MINS\": \"observed\",\n        \"SPEED_LOCAL_REGRESSION\": \"local\",\n        \"SPEED_REGIONAL_REGRESSION\": \"regional\",\n        \"SPEED_GLOBAL_REGRESSION\": \"global\",\n    }).melt(\n        id_vars=[\"id\", \"lane\", \"time\"],\n        value_vars=[\"observed\", \"regional\", \"local\", \"global\"],\n        var_name=\"measurement\",\n        value_name=\"speed\",\n    )\n\n    c_speed = altair.Chart(df_speed, width=800, height=200).mark_line().encode(\n        x=\"time:T\",\n        y=\"speed:Q\",\n        color=\"measurement:N\",\n        opacity=altair.condition(selection, altair.value(1), altair.value(0.2))\n    ).add_params(selection)\n\n    # Return the stacked chart\n    return (c_vol &amp; c_occ &amp; c_speed).interactive()\n\n# Actually make the chart!\nmake_chart(df)\n</code></pre> <p>The above snippet creates the following chart, which shows how on August 5th, some unphysically high occupancy reads caused a detector to be flagged as unreliable, resulting in our imputation schemes being used:</p> <p></p>"},{"location":"data/performance-calcs/","title":"Performance Measure Calculations","text":"<p>The PeMS Modernization project calculates the following performance measures for every single detector and station location:</p> <p>Vehicle Miles Traveled (VMT): For a selected period of time and section of the freeway, this value is the sum of the miles of freeway driven by each vehicle. For a selection of freeway of length L, the number of freeway miles driven is simply the flow for a period of time multiplied by the length L. For example, the VMT for a single station is:</p> <pre><code>VMT = Flow * Station Length\n</code></pre> <p>Vehicle Hours Traveled (VHT): For a selected period of time and section of freeway, this is the amount of time spent by all of the vehicles on the freeway. For a section of freeway of length L, that has F number of cars passing over the detector/station, it is the amount of time that these cars spent on that link. For example, the VMT for a single station is:</p> <pre><code>VHT= Flow * Station Length / Actual Speed\n</code></pre> <p>Delay: The amount of additional time spent by the vehicles on a section of road due to congestion. This is the difference between the travel time at a non-congestion speed (selected by the user) and the current speed. The congestion, or threshold, speed is usually 35MPH but we have computed the delay for a number of different thresholds to accommodate different definitions of delay. For example the delay for a single station is: The formula for this is simply:</p> <pre><code>Delay = Flow * (Station Length / Actual Speed - Station Length / Threshold Selected Speed)\n</code></pre> <p>It is important to note that delay can never be negative by definition. For delay, you have to specify a threshold which is considered to be the users definition of congested traffic. While a speed of 35 MPH is commonly used to define congestion we have calculated delay for threshold values of 35, 40, 45, 50, 55, and 60 MPH.</p> <p>Q: The sum of the VMT in a spatial and temporal region divided by the sum of the VHT in the same region. For a single location the interpretation of Q is the average speed although the value can be computed across many different temporal and spatial selections.</p> <pre><code>Q = VMT / VHT\n</code></pre> <p>Travel Time Index (TTI): The ratio of the average travel time for all users across a section of freeway to the free-flow travel time. The free-flow speed used in the PeMS Modernization project for this calculation is 60MPH. For our performance measures, the TTI is simply the free-flow speed divided by Q:</p> <pre><code>TTI = 60 / Q\n</code></pre> <p>Lost Productivity: The number of lane-mile-hours on the freeway lost due to operating under congested conditions instead of under free-flow conditions. Similar to delay, the user can select different speed threshold values of 35, 40, 45, 50, 55, and 60 MPH. When the freeway is in congestion, based on the user selected threshold, the calculation is the ratio between the measured flow and the capacity for this location. This drop in capacity is due to the fact that the freeway is operating in congested conditions instead of in free-flow. We then multiply one minus this ratio by the length of the segment to determine the number of equivalent lane-miles-hours of freeway which this represents. For the capacity we 2076 v/l/h at each location. For example, the formula for Lost Productivity during a 5-minute interval for a single station is:</p> <pre><code>Lost Productivity = 1 - (5-minute Flow / 173 v/l/5-min) * Station Length\n</code></pre> <p>It is important to note that if the actual speed is above the user selected threshold a value of 0 will be returned. A Lost Productivity value will only be calculated if the actual speed is less than the user selected threshold value.</p> <p>A couple of notes about the calculation of these performance measures:</p> <ul> <li>All performance are computed at the 5-minute level for each lane and station.</li> <li>The measures VMT, VHT and Delay can be summed up temporally and spatially.</li> <li>The measures Q and TTI are just ratios of the base performance measures of VMT and VHT.</li> </ul>"}]}