# Data Relay Documentation

# Overview
## Backgrounds
### Why we need Data Relay
* Caltrans IT infrastructure collects the road side detector signals and transforms them within the Caltrans network.
* Data Relay moves PeMS data from internal Caltrans networks to the cloud (Snowflake).
* Snowflake employs elastic computing resources to transform the data within the data warehouse, which are then consumed by PeMS users.

Therefore, Data Relay performs the critical link between the Caltrans infrastructure and the Snowflake.

The data sources needed for the PeMS system include:
* The `VDS30SEC` table.
* The sensor device configuration tables.

### How Data Relay works
* On a regular basis, Data Relay service hosted in Caltrans internal network pulls PeMS upstream data and batch them for uploading to Snowflake.
* The current upstream for Data Relay is the legacy PeMS Oracle database that serves the legacy PeMS website.

### General System Design Principle
* Data Relay is the only component in the entire PeMS modernization project that operates within Caltrans internal networks, thus there are upfront limitations we need to consider when we build Data Relay service
a) Caltrans IT team will not provide support on operating the Data Relay on a routine basis, though they will address the incidents when the hosting machines go down.
b) The legacy PeMS Oracle database was intended to only serve the legacy PeMS website frontend; supporting an additional data downstream service that powers an entire data system was not in its scope. So does the PeMS Oracle databases' internal networking, which was never purposed for directly transfer data to the public internet.
c) Cloud system's data ingestion needs to work with large batch of data in compressed form; therefore, the data relay should output that kind of data.

With those constraints set forth, there are extensive studies and experimentation done before we settled down our final architecture. Here is our high-level journey:
* We considered using Airflow, which provides a powerful tooling support for administering batch-based data pipeline workloads with reliability. The problem, however, is the constraint a), which would force us to include managing an Airflow platform as part of the PeMS modernization project scope, which would increase our cost.
* We then tried using one machine running python script with crontab as scheduling. It does not come with a heavy-weight of managing an Airflow system, and it may handle any complex data transforming operations with python libraries, but due to the constraint b above, such python script cannot simultaneously talking to Oracle and Snowflake at once.
* We then tried Kafka. Kafka as a distributed pub-sub services naturally fit into Data Relay in that it offers Task Queue management out-of-the-box, and it orchestrates the machines in and out of the PeMS Oracle networks together (as aforementioned in constraint b, only using one machine for Data Relay for pulling Oracle and pushing to Snowflake is not possible).
* In spite of Kafka's benefits, it nevertheless shares the same drawback as Airflow in that, managing a Kafka cluster has to be part of the project scope, which increases the cost. Fortunately, Kafka's administering cost is significantly lower than Airflow, in that it has low dependency, and machines are identical setup, unlike Airflow, which has to distinguish different roles such as scheduler - worker - database.
* In practice, Kafka data is consumed using pub-sub way, which may incur some minor inconveniences to certain operation such as deduplicating such as ensuring configuration tables' elements are deduplicated before sending to Snowflake. Therefore, we scope Kafka to be focused on the VDS30SEC table relaying, which accounts for 99.9% of the daily new data.


In sum, we adopted Kafka for our Data Relay service's backbone, and this design decision is the foundation for the architecture design we will introduce in the following sections.

## Whole System Components In a nutshell

1. **PeMS Oracle Database (DB96/DWO):** The source of the data that needs to be pulled and relayed.

2. **Servers**
    - **svgcmdl01 (Puller Side):**
        - **Gateway:** `/etc/systemd/system/oracle_puller.service`
        - **Configuration:** `/etc/systemd/system/oracle_puller.service.sh`
        - **Script:** `/data/projects/crawler/oracle_puller_daemon.py`
        - **Data Source:** Oracle Database (DB96/DWO)
        - **Process:** Continuously pulls data from the Oracle databases based on tasks generated by the task queue on svgcmdl02. The pulled data is then sent to the Kafka service.

    - **svgcmdl02 (Uploader Side):**
        - **Task Queue Generators:**
            - **Cron Jobs:** `append_to_crawl_queue.py` (runs every 3 hours) generate tasks for the puller on svgcmdl01, scheduling SQL queries to be executed periodically.
        - **Data Uploaders:**
            - **Script:** `aws_upload_daemon_script.sh` (runs every 6 hours) processes the pulled data and uploads it to AWS S3/Snowflake.

3. **Kafka Service**
    - **Purpose:** Centralized service to manage data flow between the task producers and consumers.
    - **Topics:**
        - **Task Queue Topic:** `oracle_crawl_queue_topic` (where tasks are published for the puller to execute)
        - **Data Buffer Topics:** `D3.VDS30SEC`, `D4.VDS30SEC`, ..., `D12.VDS30SEC` (where pulled data is buffered before being consumed by the uploader)

4. **Data Flow**
    - **Task Generation:** Cron jobs on svgcmdl02 generate tasks every few hours and send them to Kafka.
    - **Task Execution:** svgcmdl01 pulls tasks from Kafka, executes the tasks to retrieve data from the Oracle database, and then sends the data back to Kafka topics.
    - **Data Upload:** The data is consumed from Kafka topics by svgcmdl02 and uploaded to AWS S3/Snowflake at scheduled intervals.

5. **Final Destination:**
    - **AWS/Snowflake:** The final destination for the data after it has been pulled, processed, and uploaded.
---
## Diagrams
### Key Components
```plaintext
                                +-------------------+
                                | PeMS Oracle       |
                                | Database          |
                                | DB96 / DWO        |
                                +-------------------+
                                         |
                                         v
                +-------------------------------------------------+
                |           Server svgcmdl01 (Puller Side)        |
                |                                                 |
                | 1. Oracle Puller Daemon                         |
                |    - Gateway:					  |
		|	/etc/systemd/system/oracle_puller.service |
                |    - Executes: oracle_puller_daemon.py          |
                |    - Continuously pulls data from DB96/DWO      |
                |                                                 |
                | 2. Task Generation (via Kafka)                  |
                |    - Task:					  |
		|	SQL Queries from cron jobs on svgcmdl02   |
                |    - Kafka Topic: oracle_crawl_queue_topic      |
                |    - Puller consumes these tasks and pulls data |
                |                                                 |
                +-------------------------------------------------+
                                         |
                                         v
                +-------------------------------------------------+
                |           Server svgcmdl02 (Uploader Side)      |
                |                                                 |
                | 1. Cron Jobs                                    |
                |    - Generate tasks for oracle_puller           |
                |    - Example: db96 task every 3 hours           |
                |                                                 |
                | 2. Data Upload                                  |
                |    - Upload pulled data to AWS/Snowflake        |
                |    - Example: db96 data upload every 6 hours    |
                |                                                 |
                +-------------------------------------------------+
                                         |
                                         v
                +-------------------------------------------------+
                |           AWS / Snowflake                       |
                |                                                 |
                | 1. Stores the data uploaded from svgcmdl02      |
                |    - Transformed DB96 / DWO tables              |
                |                                                 |
                +-------------------------------------------------+
```

### Data Flow

```plaintext

+-------------------+       +-------------------+       +-------------------+
|                   |       |                   |       |                   |
|  svgcmdl01        |       |  Kafka Service    |       |  svgcmdl02        |
|  (Puller Side)    |       |                   |       |  (Uploader Side)  |
|                   |       |                   |       |                   |
| - Gateway         |       | - Task Queue Topic|       | - Cron Jobs       |
| - Configuration   |       | - Data Buffer     |       | - Data Uploaders  |
| - Script          |       |   Topics          |       |                   |
| - Data Source     |       |                   |       |                   |
|                   |       |                   |       |                   |
+-------------------+       +-------------------+       +-------------------+
        |                           |                          |
        |  Data pulled by           |                          |
        |  `oracle_puller`          |                          |
        |  and sent to Kafka        |                          |
        +-------------------------->|                          |
        |                           |                          |
        +---------------------------| ------------------------>|
        |                           |  Data consumed by        |
        |                           | `aws_upload_daemon_      |
        |                           |             script.sh`   |
        |                           |                          |
        +-------------------------- | ------------------------>|
            Data sent to Kafka      Data uploaded to AWS S3/Snowflake

```
---
# Workflow from Data Source to Final Destination

**From: PeMS Oracle Database**

- DB96
- DWO

```bash
ssh jupyter@svgcmdl01.dot.ca.gov
password:
```

**To： Snowflake**

- AWS S3

```bash
# account 1
jupyter@svgcmdl02.dot.ca.gov
# account 2
s159123@svgcmdl02.dot.ca.gov
password:
```

## Puller Side: `Server svgcmdl01`

### Entry point/Gateway of the program

system file automatically run by certain system software

- Gateway file: `Oracle_puller.service`

```bash
sudo ls /etc/systemd/system
# File structure
Gateway:
├── /etc/systemd/system
│   ├── oracle_puller.service
│   └── oracle_puller.service.sh
```

- Gateway Contents `Oracle_puller.service`

```bash
sudo ls /etc/systemd/system/oracle_puller.service
		# File contents
		# ----------------------------------------------------
		[Unit]
		Description=Oracle Puller Daemon
		After=network.target
		[Service]
		ExecStart=/etc/systemd/system/oracle_puller.service.sh
		WorkingDirectory=/data/projects/crawler
		Restart=always
		User=jupyter

		[Install]
		WantedBy=multi-user.target
		# ----------------------------------------------------

```

- Gateway Configuration: `oracle_puller.service.sh`

```bash
/etc/systemd/system/oracle_puller.service.sh
		# File contents
		# ----------------------------------------------------
		#!/user/bin/bashrc
		source /home/jupyter/.bashrc
		cd /data/projects/crawler  # working folder
		python3.9 oracle_puller_daemon.py # execute the program
		# ----------------------------------------------------
```

- Puller file structure

```bash

├── /data/projects/crawler
│   └── oracle_puller_daemon.py
		# running continuously to pull data from Oracle
```

### Operations on the program

- check status of the system control file: `oracle_puller.service`

```bash
sudo systemctl status oracle_puller
	# key info
	oracle_puller.service - Oracle Puller Daemon
	CGroup: /system.slice/oracle_puller.service
	│   ├── /user/bin/bash /etc/systemd/system/oracle_puller.service.sh
	│   └── python3.9 oracle_puller_daemon.py
```

- Operations : **Stop** and **Restart**

```bash
sudo systemctl stop oracle_puller
sudo systemctl restart oracle_puller
```

- For new service, redefine the similar contents like the `oracle_puller.service`

```bash
sudo vi /etc/systemd/system/oracle_puller.service
```

## Uploader Side: `Server svgcmdl02`

### Cron Jobs  - Overview

- **Note:** For the crontab , use the s159123 account

```bash
crontab -e
# Service health checking, done at weekdays morning 730am
34 7 * * 1-5 /home/s159123/check_services.sh
 
# Every hour, clean the json files under tmp that are 4 hours old
0 * * * * find /tmp -name "*.json" -type f -mmin +240 -exec rm {} \; 2>/dev/null
# ======================================== Task Queue Generator ==================================================================================================
# DWO config data upload daily (upload config data to AWS)
# db96 (generate the task queues for oracle_puller)
0 2,5,8,11,14,17,20,23 * * * cd /home/s159123/gitrepo/dataop/agents/agent_oracle_puller_queue && python3 append_to_crawl_queue.py --append_to_queue --githash '3110c63' --day_partition_number 8
	# explanation -------------------------------------------------------------------
		# 0: minutes
		# 2,5,8,11,14,17,20,23: hours, 8 times in a day for the crontab job to
																   # launch the following script
		# * * *: every day; day, month, year
		# cd /home/s159123/gitrepo/dataop/agents/agent_oracle_puller_queue: go to the working folder
		# python3 append_to_crawl_queue.py --append_to_queue --githash '3110c63' --day_partition_number 8
	# purpose:
		# sending the controlling commands to pull the DB96 every 3 hours
	#---------------------------------------------------------------------------------

# DWO config crawl task daily
0 19 * * * cd /nfsdata/dataop/uploader && python3.9 append_to_crawl_queue_config.py --githash 'ccccccc' --append_to_queue
# ========================================Data Uploader ==================================================================================================
# DWO config data upload daily (upload config data to AWS)
0 23 * * * sh /nfsdata/dataop/uploader/aws_upload_daemon_script.sh "CONTROLLER_CONFIG,CONTROLLER_CONFIG_LOG,DETECTOR_CONFIG,DETECTOR_CONFIG_LOG,STATION_CONFIG,STATION_CONFIG_LOG"
 
# Cron scheduling the aws uploader, every 6 hours. (consume the data pulled from the oracle_puller, upload PeMS data to AWS)
0 0,6,12,18 * * * sh /nfsdata/dataop/uploader/aws_upload_daemon_script.sh "D3.VDS30SEC,D4.VDS30SEC,D5.VDS30SEC,D6.VDS30SEC,D7.VDS30SEC,D8.VDS30SEC,D10.VDS30SEC,D11.VDS30SEC,D12.VDS30SEC"
 
 
# 0 23 * * * sh /nfsdata/dataop/uploader/aws_upload_daemon_script.sh "D3.VDS30SEC,D4.VDS30SEC,D5.VDS30SEC,D6.VDS30SEC,D7.VDS30SEC,D8.VDS30SEC,D10.VDS30SEC,D11.VDS30SEC,D12.VDS30SEC,CONTROLLER_CONFIG,CONTROLLER_CONFIG_LOG,DETECTOR_CONFIG,DETECTOR_CONFIG_LOG,STATION_CONFIG,STATION_CONFIG_LOG"
```

### Task Queue Generators on svgcmdl02 (D2)

**Result/Output**

- **Crawl Queue:** [task1, task2, task3, ...], [task_n, task_n+1, ...]

**Relation with** `oracle_puller` :

- **`oracle_puller`**: task (SQL statement) executor/consumer (svgcmdl01)
- **`crontab controller`**: task (SQL statement)  generator/ producer (svgcmdl02)

#### **db96 Task Generator** (every 3 hours)

```bash
# db96 Task Generator file path
cd /home/s159123/gitrepo/dataop/agents/agent_oracle_puller_queue
python3 append_to_crawl_queue.py
```

- **Cron Scheduling Command**

```bash
# db96
0 2,5,8,11,14,17,20,23 * * * cd /home/s159123/gitrepo/dataop/agents/agent_oracle_puller_queue && python3 append_to_crawl_queue.py --append_to_queue --githash '3110c63' --day_partition_number 8
```

- **Purpose**
    - Populate/Produce the tasks queues for `oracle_puller` to pull data the DB96 every 3 hours
- **Explanation**
    - `0`: minutes
    - `2,5,8,11,14,17,20,23`: hours, 8 times in a day for the crontab job to  launch the following script
    - `* * *`: every day; day, month, year
    - `cd /home/s159123/gitrepo/dataop/agents/agent_oracle_puller_queue`: go to the working folder
    - `python3 append_to_crawl_queue.py --append_to_queue --githash '3110c63' --day_partition_number 8` : find and append the task queue  every 3 hours for `oracle_puller.service`   (running continuously) to pull data from the `DB96`
- **More details about** `append_to_crawl_queue.py`
    - **Purpose:** Populate the crawl queue with new SQL statements. This process is similar to creating a table in a spreadsheet and populating it, but instead of storing the queue in Excel, it's stored in a Kafka service.
    - **Service topic**:`SeCRAW_QUEUE_KAFKA_TOPIC = "oracle_crawl_queue_topic"`
        - The same topic corresponds to the same task queue
        - Kafka service distributes the tasks
    - **Server cluster:** `KAFKA_SERVERS=(PLAINTEXT://svgcmdl03:9092, PLAINTEXT://svgcmdl04:9092,PLAINTEXT://svgcmdl05:9092)`
    - **SQL template:**

        - **Purpose:** SQL command that enables the puller to extract data from the Oracle database
        - **Interpretation:** Each SQL query represents a distinct task to be executed
        - **On the puller side:** The system performs parameter replacement to generate the final SQL query. Read the SQL statement in the SQL queue one by one, and consume it.
        - **Google Drive Excel Example (task queue):** https://drive.google.com/drive/u/0/home

#### **DWO Task Generator (Daily)**

```bash
# DWO Task Generator file path
cd /home/s159123/gitrepo/dataop/agents/agent_oracle_puller_queue
python3 append_to_crawl_queue_config.py
```

- **Cron Scheduling Command**

```bash
# DWO config crawl task daily
0 19 * * * cd /nfsdata/dataop/uploader && python3.9 append_to_crawl_queue_config.py --githash 'ccccccc' --append_to_queue
```

- **Purpose**
    - Populate/Produce the task queues for `oracle_puller` to pull configuration data from the DWO database daily
- **Explanation**
    - refer other similar detailed explanation to the db96

    Some other commands for testing

    ```bash
    python3.9 append_to_crawl_queue.py --output_csv_file db96.text.csv --day_partition_number 8 --crawl_window_start "2024-03-01 18:00:00" --githash default
    vi db96.text.csv
    python3.9 append_to_crawl_queue.py --output_csv_file dwo.text.csv --day_partition_number 8 --crawl_window_start "2024-03-01 18:00:00" --githash default
    vi dwo.text.csv
    ```


### Data Uploaders/Consumers on svgcmdl02 (D2)

**Result/Output**

- Data uploaded to AWS/Snowflake

**Relation with** `oracle_puller` :

- **`oracle_puller`**: Data (json) generator/ producer (svgcmdl01)
- **`crontab controller`**: Data (json) executor/consumer (svgcmdl02)

#### **db96 Data: Uploader for AWS** (every 3 hours)

```bash
# gateway configuration
/nfsdata/dataop/uploader/aws_upload_daemon_script.sh
```

- **Cron Scheduling Command**

```bash
# Cron scheduling the aws uploader, every 6 hours. (consume the data pulled from the oracle_puller, upload PeMS data to AWS)
0 0,6,12,18 * * * sh /nfsdata/dataop/uploader/aws_upload_daemon_script.sh "D3.VDS30SEC,D4.VDS30SEC,D5.VDS30SEC,D6.VDS30SEC,D7.VDS30SEC,D8.VDS30SEC,D10.VDS30SEC,D11.VDS30SEC,D12.VDS30SEC"
```

- **Purpose**
    - Upload the data extracted by `oracle_puller` to AWS/Snowflake every 6 hours
- **Explanation**
    - refer other similar detailed explanation to the db96 task generator

#### **DWO C**onfig Data **Uploader for AWS** (every 3 hours)

```bash
/nfsdata/dataop/uploader/aws_upload_daemon_script.sh
```

- **Cron Scheduling Command**

```bash
# DWO config data upload daily (upload config data to AWS)
0 23 * * * sh /nfsdata/dataop/uploader/aws_upload_daemon_script.sh "CONTROLLER_CONFIG,CONTROLLER_CONFIG_LOG,DETECTOR_CONFIG,DETECTOR_CONFIG_LOG,STATION_CONFIG,STATION_CONFIG_LOG"
```

- **Purpose**
    - Upload the config data extracted by `oracle_puller` to AWS/Snowflake daily
- **Explanation**
    - refer other similar detailed explanation to the db96  task generator

## Kafka Service

### Purpose

- Centralized service to manage the supplier and consumer
- Hosting the crawl queue

```bash
cd /home/s159123/gitrepo/dataop/agents/agent_oracle_puller_queue
```

- Perform a health check for the Kafka service by verifying if each server is running

    ```bash
    telnet svgcmd102/3/4/5 9092
    ```


### Different Kafka Topics

#### 1. kafka topic for crawl tasks

`oracle_crawl_queue_topic` (coordination between the crawl task generator and puller)

#### 2. kafka topics for data buffer

- Kafka topic: `D3.VDS30SEC`
- Kafka topic: `D4.VDS30SEC`
- …
- Kafka topic: `D12.VDS30SEC`

**Puller (every 10 minutes):**

**Uploader (every 6 hours)**

1. `df = pd.read_sql("select * from D3.VDS30SEC …")` (Notes: * is too heavy, 10-minute data every time, small batch pulling continuously)
2. puller then transfer the `df` then upload to kafka topic: `D3.VDS30SEC` (in the format of json)

Uploader on D2 svgcml02 will be able to consume the data in `D3.VDS30SEC` and batch uploading (with transform) it to snowflake’s `D3.VDS30SEC`

## Source of Data Relay [Task Details]

#### PeMS Oracle Database (DB96, DWO)

- **DB96** (Headquarter database for storing the field sensor data from district TMCs)
    - Data resource
        - `D3.VDS30SEC`
        - `D4.VDS30SEC`
        - …
        - `D12.VDS30SEC`
    - Columns for D3.VDS30SEC:
        - `SAMPLE_TIME`
        - `LOOP1_SPD`
        - `LOOP1_VOL`
        - `LOOP2_SPD`
        - `LOOP2_VOL`
        - …
        - `LOOP8_SPD`
        - `LOOP8_VOL`
    - Credentials in `/home/jupyter/.bashrc on svgcmdl01` (i.e. `DB_USER` `DB96_PASSWORD`)
    - Test some simple query `query_db96_debug.py`:
        - `cd /data/projects/crawler/agent_oracle_puller_queue/`
        - `python3.9 query_db96_debug.py`
            - `Get_Data_From_PeMS(dbname: DB96/DWO,  sql statement)` :
            - To stop the process: Use `ps -ef | grep query` to find the process ID, then `kill [process_id]` (e.g., `kill 66472`)
- **DWO**
    - Six Tables
        - `PEMS.STATION_CONFIG`
        - `PEMS.STATION_CONFIG_LOG`
        - `PEMS.CONTROLLER_CONFIG`
        - `PEMS.CONTROLLER_CONFIG_LOG`
        - `PEMS.DETECTOR_CONFIG`
        - `PEMS.DETECTOR_CONFIG_LOG`
    - Credentials in `/home/jupyter/.bashrc on svgcmdl01` (i.e. `DB_USER` `DB96_PASSWORD`)
- Other Path:
    - SQL Developer (Ken)

## Target of Data Relay [Task Details]

**Purpose:** Check the output/target of the data relay

#### **How to navigate**

- Login to [`ssh s159123@svgcmdl02.dot.ca.gov`](mailto:to`s159123@svgcmdl02.dot.ca.gov)
- `cd /nfsdata/dataup/uploader/`
- Write some SQL statement such as the ones in `snowsql/scratch.sql` (exemplary SQL statements)
- Procedure to run:
    - **Step 1:** Load snowflake credentials `SNOWSQL_PWD` in `/home/s159123/.bashrc` to the environment:

        ```bash
        source /home/s159123/.bashrc
        ```

    - **Step 2.0:** Use Snowsql command (test):

        ```bash
        REQUESTS_CA_BUNDLE=/etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt
        snowsql -a NGB13288 -u MWAA_SVC_USER_DEV -o insecure_mode=True -f
        /nfsdata/dataop/uploader/snowsql/scratch.sql -d RAW_DEV
        ```

    - **Step 2.1:** Use Snowsql command (real program):

        ```bash
        REQUESTS_CA_BUNDLE=/etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt
        snowsql -a NGB13288 -u MWAA_SVC_USER_DEV -o insecure_mode=True -f
        /nfsdata/dataop/uploader/snowsql/snowsql_load_CONTROLLER_CONFIG_LOG.sql -d RAW_DEV
        ```

- Snowflake DBs (Transformed data)
    - Transformed DB96 tables:
        - DB96.VDS30SEC
    - Transformed DWO tables:
        - Six Tables
            - `DB96.CONTROLLER_CONFIG`
            - `DB96.CONTROLLER_CONFIG_LOG`
            - `DB96.STATION_CONFIG`
            - `DB96.STATION_CONFIG_LOG`
            - `DB96.DETECTOR_CONFIG`
            - `DB96.DETECTOR_CONFIG_LOG`
    - website: `https://app.snowflake.com/vsb79059/dse_caltrans_pems/worksheets`

### **Intermediate data** [Task Details]

- Read the local parquet file: `/nfsdata/dataop/uploader/tmp/CONTROLLER_CONFIG_LOG_dump_static.parquet @~/controller_config_log;`
    - Read this parquet file `pd.read_parquet(/nfsdata/dataop/uploader/tmp/CONTROLLER_CONFIG_LOG_dump_static.parquet, dtype_backend='pyarrow')`
- Transform the parquet file to snowflake tables

# FAQ
### Foundational concepts
---

1. **Cron Jobs:**
   - **Definition:** Cron jobs are scheduled tasks that run automatically at specified intervals. In this workflow, they are responsible for generating task queues that trigger data pulls from the Oracle database.
   - **Example:** A cron job might be set to run every 3 hours to initiate a data pull process via Kafka.

2. **Kafka:**
   - **Definition:** Kafka is a distributed event streaming platform used for building real-time data pipelines and streaming applications. It allows different services to communicate asynchronously by sending messages through topics.
   - **In this context:** Kafka serves as a central service for managing tasks and buffering data between producers (task generators) and consumers (data processors).

3. **Oracle Puller:**
   - **Definition:** A custom script or daemon (e.g., `oracle_puller_daemon.py`) that extracts data from an Oracle database based on predefined tasks (e.g., SQL queries). The data is then sent to Kafka for further processing.
   - **Function:** It continuously pulls data from Oracle as tasks are generated, ensuring up-to-date data is retrieved.

4. **Task Queue:**
   - **Definition:** A list or queue of tasks that need to be processed by a specific service. In this case, the task queue is generated by cron jobs and sent to Kafka to be consumed by the `oracle_puller`.
   - **In this context:** The tasks in the queue are SQL queries that the `oracle_puller` executes to retrieve data from Oracle.

5. **Data Buffer:**
   - **Definition:** A temporary storage area where data is held before being processed or transferred to its final destination. In this workflow, Kafka acts as a buffer for the data pulled from Oracle, ensuring smooth data flow to the uploader.
   - **Purpose:** Buffers prevent data loss or overload by holding data temporarily, especially during peak loads or system delays.

6. **AWS S3:**
   - **Definition:** Amazon Simple Storage Service (S3) is an object storage service used for storing and retrieving any amount of data at any time. It's scalable, reliable, and widely used for cloud data storage.
   - **In this workflow:** Data pulled from Oracle is uploaded to AWS S3 as a final storage destination after being processed.

7. **Snowflake:**
   - **Definition:** Snowflake is a cloud-based data warehousing platform that allows organizations to store, process, and analyze large volumes of structured and semi-structured data.
   - **In this workflow:** Snowflake is used as another storage destination for the pulled data, enabling further analysis and reporting.

8. **SQL Queries:**
   - **Definition:** Structured Query Language (SQL) is used to communicate with and manipulate databases. SQL queries retrieve, update, or manage data stored in relational databases.
   - **In this context:** SQL queries are generated by cron jobs and sent to the `oracle_puller`, which uses them to extract data from the Oracle database.

9. **Daemon:**
   - **Definition:** A daemon is a background process that runs continuously and performs specific operations without user interaction.
   - **In this context:** The `oracle_puller_daemon.py` script is a daemon that constantly monitors for tasks from Kafka, pulls data from Oracle, and forwards it to Kafka topics.

10. **Buffer Topics (Kafka Topics):**
    - **Definition:** In Kafka, topics are named channels where data records (messages) are published and from which consumers read. Topics are partitioned and distributed for scalability.
    - **In this workflow:** Buffer topics (e.g., `D3.VDS30SEC`) temporarily hold the data pulled from Oracle before it is uploaded to AWS or Snowflake.

11. **Producers and Consumers:**
    - **Producer:** In Kafka, a producer is a service or application that sends data to a Kafka topic.
    - **Consumer:** A consumer reads data from a Kafka topic and processes it further.
    - **In this workflow:** svgcmdl02 acts as a producer of task queues, while svgcmdl01 is a consumer of tasks (SQL queries). Similarly, svgcmdl01 produces data, and svgcmdl02 consumes it for uploading.

### Task Related Questions
---

1. **What is the purpose of the `oracle_puller_daemon.py` script?**
   - The `oracle_puller_daemon.py` script is used to pull data from the Oracle database. It operates continuously to retrieve and process data based on tasks assigned via Kafka.

2. **How often do cron jobs generate tasks for the `oracle_puller`?**
   - Cron jobs generate tasks every 3 hours for the `oracle_puller` to execute. This scheduling ensures that data is pulled from the Oracle database at regular intervals.

3. **What role does Kafka play in the data workflow?**
   - Kafka serves as a central messaging service that manages the flow of tasks and data between different components. It stores task queues and buffers data before it is consumed by the uploader.

4. **How is data transferred from Kafka to AWS S3/Snowflake?**
   - Data is transferred from Kafka to AWS S3/Snowflake by the `aws_upload_daemon_script.sh`, which runs every 6 hours. This script consumes the data buffered in Kafka topics and uploads it to the final storage destinations.

5. **What is the significance of the data buffer topics in Kafka?**
   - Data buffer topics in Kafka (e.g., `D3.VDS30SEC`, `D4.VDS30SEC`) are used to temporarily store the pulled data before it is processed and uploaded. These topics help manage data flow and ensure that data is handled efficiently.

6. **How do cron jobs interact with Kafka?**
   - Cron jobs on svgcmdl02 generate SQL tasks and publish them to Kafka topics. These tasks are then consumed by the `oracle_puller` on svgcmdl01, which pulls the data from the Oracle database.

7. **What happens if there is a failure in the data pull process?**
   - In case of a failure, the `oracle_puller` will not be able to retrieve data, which could affect the subsequent steps. Error handling and retry mechanisms should be in place to address failures and ensure data integrity.

8. **How is the scheduling of data uploads managed?**
   - Data uploads are managed by the `aws_upload_daemon_script.sh`, which is scheduled to run every 6 hours. This script handles the transfer of data from Kafka to AWS S3/Snowflake.

9. **Can the frequency of cron jobs or data uploads be adjusted?**
   - Yes, the frequency of cron jobs and data uploads can be adjusted by modifying the respective cron job schedules and script settings as needed.

10. **What are the key components involved in this data workflow?**
    - The key components are the Oracle database, `oracle_puller` script, Kafka service, cron jobs, and AWS S3/Snowflake. Each plays a crucial role in managing and processing the data.

